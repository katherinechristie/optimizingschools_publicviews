{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages and imports\n",
    "\n",
    "## outputs and loading\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "## dataframe\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "## preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## add punctuation and some application-specific words\n",
    "## to stopword list\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "## lda\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "\n",
    "## functions\n",
    "stopwords_standard = set(stopwords.words('english'))\n",
    "def remove_stop(row, colname, stopword_dict):\n",
    "    \n",
    "    string_of_col = str(row[colname])\n",
    "    try:\n",
    "        processed_string = \" \".join([i for i in wordpunct_tokenize(string_of_col) if \n",
    "                        i not in stopword_dict])  ## removed numeric\n",
    "        return(processed_string)\n",
    "    except:\n",
    "        processed_string = \"\" # to handle data errors where not actually text\n",
    "        return(processed_string)\n",
    "    \n",
    "def processtext(row, colname):\n",
    "    \n",
    "    string_of_col = str(row[colname])\n",
    "    try:\n",
    "        processed_string = \" \".join([porter.stem(i.lower()) for i in wordpunct_tokenize(string_of_col) if \n",
    "                        i.lower().isalpha() and len(i) >=3])  \n",
    "        return(processed_string)\n",
    "    except:\n",
    "        processed_string = \"\" # to handle data errors where not actually text eg someone left blank\n",
    "        return(processed_string)\n",
    "    \n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyze free responses from optimizing schools pilot\n",
    "\n",
    "*Note of caution*: these are based on low N and are meant to be illustrative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdata_schools = pd.read_csv(\"../data/cleaned_fr_pilot.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdata_schools['text_lower'] = frdata_schools.explain_fairness.astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to lowercase and remove stopwords\n",
    "frdata_schools['text_lower_nostop'] = frdata_schools.apply(remove_stop,\n",
    "                                    axis = 1,\n",
    "                                   args = [\"text_lower\", stopwords_standard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove punctuation, digits, and length2 or less\n",
    "frdata_schools['text_preprocess'] = frdata_schools.apply(processtext,\n",
    "                                    axis = 1,\n",
    "                                   args = [\"text_lower_nostop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store in a list and re-tokenize\n",
    "all_stemmed_text = frdata_schools.text_preprocess\n",
    "text_preprocess_tokens = [wordpunct_tokenize(one_row) for one_row \n",
    "                         in all_stemmed_text]\n",
    "\n",
    "## create dictionary (all unique words and counts)\n",
    "dictionary = corpora.Dictionary(text_preprocess_tokens)\n",
    "\n",
    "## filter out words that are in almost none or almost all documents\n",
    "lower_thres = 0.01*frdata_schools.shape[0]\n",
    "upper_thres = 0.99*frdata_schools.shape[0]\n",
    "dictionary.filter_extremes(no_below=lower_thres, no_above=upper_thres)\n",
    "\n",
    "## use the dictionary to create the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_preprocess_tokens]\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimate\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word=dictionary, passes=10,\n",
    "                                          alpha = 'auto',\n",
    "                                          per_word_topics = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el770591404901720847361594929250\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el770591404901720847361594929250_data = {\"mdsDat\": {\"x\": [0.0037323067663884183, -0.06417031847596501, -0.0679998405965356, -0.10978514367140733, -0.0203405283983291, -0.06537083777754056, 0.07228166059192992, -0.04597643322181709, 0.09238808899552804, 0.20524104578774804], \"y\": [-0.05379804987480537, 0.08152673590245163, -0.05104094352829556, -0.008104243139709597, 0.12389531072304831, -0.07893639556167266, -0.12468001996048085, 0.032541150878923175, 0.0637215905479884, 0.01487486401255295], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [15.706087940196564, 14.161443480385616, 12.373803544897797, 12.23241601244439, 11.852669424562775, 8.7353395231808, 8.107968186878008, 7.571872303260162, 5.1676777371972555, 4.090721846996635]}, \"tinfo\": {\"Term\": [\"parent\", \"bia\", \"incom\", \"take\", \"help\", \"counselor\", \"factor\", \"request\", \"make\", \"draw\", \"decis\", \"need\", \"would\", \"human\", \"think\", \"better\", \"algorithm\", \"consider\", \"account\", \"name\", \"kid\", \"random\", \"child\", \"data\", \"predict\", \"peopl\", \"base\", \"mentor\", \"mani\", \"bias\", \"famili\", \"without\", \"emot\", \"figur\", \"success\", \"least\", \"data\", \"point\", \"low\", \"set\", \"assist\", \"attend\", \"base\", \"program\", \"other\", \"provid\", \"household\", \"account\", \"far\", \"abl\", \"decis\", \"know\", \"choic\", \"live\", \"everi\", \"differ\", \"incom\", \"import\", \"best\", \"need\", \"person\", \"judgement\", \"student\", \"factor\", \"help\", \"like\", \"use\", \"mentor\", \"inform\", \"think\", \"get\", \"make\", \"take\", \"algorithm\", \"could\", \"human\", \"fair\", \"model\", \"school\", \"predict\", \"would\", \"request\", \"parent\", \"involv\", \"care\", \"circumst\", \"come\", \"fall\", \"home\", \"judgment\", \"children\", \"less\", \"educ\", \"problem\", \"given\", \"might\", \"etc\", \"struggl\", \"child\", \"enough\", \"abl\", \"risk\", \"mentor\", \"mayb\", \"receiv\", \"actual\", \"chosen\", \"opinion\", \"though\", \"want\", \"even\", \"think\", \"model\", \"one\", \"well\", \"bia\", \"student\", \"need\", \"would\", \"use\", \"incom\", \"algorithm\", \"predict\", \"fair\", \"determin\", \"good\", \"base\", \"may\", \"equal\", \"wrong\", \"believ\", \"lead\", \"futur\", \"everi\", \"name\", \"chanc\", \"pick\", \"result\", \"predictor\", \"made\", \"given\", \"randomli\", \"draw\", \"truli\", \"other\", \"miss\", \"sure\", \"keep\", \"measur\", \"activ\", \"line\", \"number\", \"opinion\", \"opportun\", \"control\", \"regardless\", \"statist\", \"point\", \"algorithm\", \"probabl\", \"everyon\", \"predict\", \"student\", \"help\", \"person\", \"base\", \"need\", \"would\", \"model\", \"data\", \"use\", \"random\", \"fair\", \"get\", \"kid\", \"better\", \"parent\", \"school\", \"mentor\", \"random\", \"opportun\", \"draw\", \"unfair\", \"though\", \"creat\", \"neither\", \"individu\", \"time\", \"counselor\", \"name\", \"want\", \"choos\", \"probabl\", \"give\", \"randomli\", \"situat\", \"benefit\", \"help\", \"lot\", \"would\", \"program\", \"howev\", \"actual\", \"children\", \"accur\", \"sure\", \"select\", \"fail\", \"limit\", \"school\", \"kid\", \"think\", \"student\", \"model\", \"mentor\", \"fair\", \"algorithm\", \"predict\", \"might\", \"determin\", \"use\", \"realli\", \"like\", \"need\", \"may\", \"fact\", \"counsel\", \"year\", \"plenti\", \"thing\", \"issu\", \"still\", \"due\", \"error\", \"money\", \"identifi\", \"child\", \"problem\", \"combin\", \"judgement\", \"educ\", \"comput\", \"good\", \"far\", \"struggl\", \"howev\", \"sens\", \"possibl\", \"differ\", \"may\", \"life\", \"feel\", \"mayb\", \"like\", \"behind\", \"way\", \"counselor\", \"human\", \"could\", \"parent\", \"school\", \"kid\", \"get\", \"know\", \"think\", \"use\", \"bias\", \"inform\", \"need\", \"algorithm\", \"would\", \"student\", \"fair\", \"make\", \"might\", \"realli\", \"actual\", \"judg\", \"control\", \"regardless\", \"gener\", \"someth\", \"receiv\", \"show\", \"object\", \"determin\", \"benefit\", \"realli\", \"chosen\", \"limit\", \"fail\", \"assign\", \"miss\", \"drawn\", \"attend\", \"fall\", \"incom\", \"probabl\", \"cutoff\", \"could\", \"predictor\", \"care\", \"household\", \"well\", \"give\", \"better\", \"poor\", \"way\", \"fair\", \"need\", \"school\", \"think\", \"mentor\", \"help\", \"student\", \"would\", \"factor\", \"use\", \"get\", \"algorithm\", \"much\", \"good\", \"predict\", \"find\", \"allow\", \"servic\", \"line\", \"put\", \"variabl\", \"assign\", \"case\", \"improv\", \"effici\", \"play\", \"rather\", \"better\", \"everyon\", \"resourc\", \"poor\", \"relat\", \"whole\", \"accur\", \"keep\", \"behind\", \"taken\", \"certain\", \"drawn\", \"mani\", \"consider\", \"account\", \"elimin\", \"system\", \"take\", \"kid\", \"alway\", \"predict\", \"model\", \"data\", \"chanc\", \"incom\", \"factor\", \"school\", \"fair\", \"mentor\", \"may\", \"need\", \"get\", \"student\", \"help\", \"would\", \"way\", \"remov\", \"favor\", \"method\", \"whole\", \"error\", \"measur\", \"enough\", \"also\", \"bias\", \"cutoff\", \"certain\", \"activ\", \"fail\", \"class\", \"limit\", \"opinion\", \"say\", \"number\", \"much\", \"hard\", \"system\", \"avail\", \"gener\", \"provid\", \"past\", \"make\", \"howev\", \"judgment\", \"possibl\", \"truli\", \"accur\", \"fair\", \"base\", \"predict\", \"counselor\", \"incom\", \"might\", \"model\", \"parent\", \"use\", \"random\", \"student\", \"would\", \"request\", \"may\", \"help\", \"bia\", \"algorithm\", \"need\", \"way\", \"think\", \"object\", \"peopl\", \"elimin\", \"ask\", \"even\", \"choos\", \"mani\", \"look\", \"alway\", \"relat\", \"servic\", \"sure\", \"lead\", \"futur\", \"year\", \"chosen\", \"least\", \"success\", \"figur\", \"judgment\", \"variabl\", \"decid\", \"human\", \"bias\", \"bia\", \"system\", \"pick\", \"judgement\", \"remov\", \"unfair\", \"person\", \"get\", \"algorithm\", \"would\", \"use\", \"factor\", \"child\", \"way\", \"counselor\", \"school\", \"kid\", \"student\", \"know\", \"help\", \"consider\", \"varieti\", \"result\", \"bia\", \"neither\", \"select\", \"sens\", \"take\", \"relat\", \"keep\", \"behind\", \"decis\", \"account\", \"elimin\", \"wrong\", \"give\", \"make\", \"factor\", \"method\", \"comput\", \"counsel\", \"one\", \"human\", \"also\", \"alway\", \"less\", \"person\", \"certain\", \"well\", \"kid\", \"algorithm\", \"think\", \"need\", \"student\", \"request\", \"mentor\", \"better\", \"could\"], \"Freq\": [56.0, 21.0, 52.0, 20.0, 66.0, 34.0, 24.0, 19.0, 22.0, 17.0, 15.0, 87.0, 71.0, 15.0, 56.0, 19.0, 50.0, 8.0, 11.0, 15.0, 28.0, 19.0, 16.0, 23.0, 35.0, 8.0, 23.0, 51.0, 10.0, 11.0, 5.600588130637387, 4.952892763816626, 3.8740485211055335, 1.9874125363117998, 1.9874050221987725, 1.9873420061145204, 12.475894986876876, 1.9874121947612078, 1.9872931643798426, 2.384686424472473, 2.8314082996119847, 1.826973073480103, 10.51328585456717, 2.9124719171533933, 3.314005282748797, 1.987412707087096, 1.9872856502668153, 4.8349891578550865, 1.505119020876115, 4.778842680662959, 6.0033425395997355, 5.346578321456148, 1.7580602890298058, 2.4436602574664366, 1.9872414194651316, 1.9607148927256293, 17.769870418534477, 1.2436418057110035, 1.8321694241891069, 28.07745879518034, 4.036791913456593, 2.927510048175263, 33.16312798549377, 6.88523504885217, 14.258036300406374, 5.408773318084699, 7.993508656785466, 8.186106303295368, 3.4289541345376273, 7.877809076795085, 5.098982490844935, 4.643549026301989, 4.335844731629683, 6.331427841206948, 4.452959697273943, 3.6067991863206976, 4.894294294923437, 4.617707991600937, 4.294884276865828, 3.9562464281634795, 4.221342286464796, 14.499453085856615, 33.362748495248105, 3.580528710438928, 2.54461831844163, 1.9965345327794535, 4.991872875543097, 1.9817156437027987, 2.8231530420947193, 2.9512549232515677, 4.543567566861313, 4.070750784904075, 2.001706415761474, 1.9999927714329369, 1.9919848833073277, 8.548568856906428, 2.242629964673932, 2.8734885148614993, 6.184138183976393, 1.6502413361422859, 4.457635286781875, 2.0101195794619517, 18.239637891701754, 1.5310382718633977, 2.071212100731886, 5.508894875256986, 1.047085496422097, 1.0484613854415636, 1.048480324991605, 1.04848078693185, 2.0133371472478365, 15.130026156972827, 12.296905984193751, 4.516888977916902, 2.9707971513324347, 4.762720949891699, 21.232801265812277, 12.404498642357053, 10.31076988030732, 7.291426196796563, 6.806950658978105, 6.211817643451444, 4.827287877723689, 5.10506640519586, 2.764797670755963, 2.760624810543603, 2.945930599988727, 2.8499283305303984, 5.637965781292126, 2.8579300513151344, 3.7968338083740525, 1.9454301706618313, 1.944834549991929, 2.873963508562087, 8.067924285285143, 7.079792419314571, 3.9516214531320926, 1.9472617008582127, 1.9473074453867347, 1.9277123729661538, 1.947306369044887, 4.531544484443888, 7.062812050327214, 1.9473047545321156, 2.451335729427721, 3.774114384655421, 1.0199855975072913, 1.0199855975072913, 1.0199859338641188, 1.019986673849139, 1.01998680839187, 1.0199870102059663, 1.0199857993213879, 1.0190697651377352, 1.0199862702209461, 1.0197307063035124, 1.0199870102059663, 1.0176514829394576, 12.685009423552353, 1.962863679567018, 1.945029636951802, 8.598403132059383, 27.156038081144967, 14.08930294500798, 2.872594670817432, 4.556440271378318, 13.153602381287447, 10.262107954882968, 7.4548598812301, 3.8913132121560303, 5.4468779695536655, 3.3269406296870874, 5.2619177719534544, 3.8685633817802043, 3.8178598772809855, 2.779575862922615, 3.4481577103901597, 2.651985762140046, 2.595533515832924, 10.2487500404373, 1.940334607672852, 9.154561454887926, 2.831156344711385, 1.8921982253258824, 1.8738558500027427, 1.5508066218491252, 1.944828062014669, 1.9441581138285533, 14.06361190935838, 5.9582842797812745, 1.4023527806768337, 2.0010991874648787, 2.730699762240505, 3.7161527113597073, 3.9061727342639707, 1.9441577148123614, 2.8699705117681695, 21.873984641833818, 2.0097043706583095, 22.028462430454464, 1.944157448801567, 1.9010179462251315, 5.483942875277341, 3.039349427068047, 2.9535199801579934, 1.0183377355650998, 2.0380333232310646, 1.0183376025597026, 1.0183374030516066, 8.74586459820234, 6.21357217921878, 9.343030102838277, 17.891051832689744, 6.979092723606308, 6.975948476014532, 6.247719984917279, 6.220519849151268, 5.093412959795879, 3.5570410146977123, 2.654543531864067, 3.311432982063686, 2.37038601291385, 2.5126084182180244, 3.103140677680651, 2.5684661628934777, 2.8820329820419213, 3.7948883273842373, 1.920091087424148, 1.9486466047185544, 3.79997069473054, 2.8196292539198407, 3.6708868637707632, 2.488844671681743, 1.9555048880307773, 1.960795648418944, 1.9607943596555435, 7.019737529018858, 1.96248689262945, 1.9554856854561093, 3.8258676220060073, 1.941261990433558, 2.6221587396273955, 5.307018911722393, 1.5028978833368094, 2.9706411363868344, 2.4418053230680163, 1.960820392676234, 1.9459130086697052, 1.9412595417830971, 8.947248156216157, 1.796740964833298, 2.9464054251346408, 1.4760428893493929, 5.911438120955079, 1.0270655578966577, 5.952010454824982, 9.484935712075126, 4.309698013109804, 6.314384432278657, 11.265865483129108, 7.714762991372629, 6.274115215560558, 5.711414308613518, 3.664547952108951, 7.991683191780511, 6.808019477580952, 3.262596308377897, 3.2690254334777458, 7.7661686703873265, 5.996084101101425, 6.626462613758164, 8.166234853263417, 5.55750709029091, 3.994394056838788, 3.637105797516256, 3.2340383424330295, 3.082266605065087, 1.965752376634166, 1.8946136850679336, 1.8680019039434852, 1.9657529465202634, 1.966629431337939, 2.7852331976535023, 1.7490138659881018, 1.9323502126981358, 4.382431686860352, 2.8357363136358327, 3.835857350238789, 1.0309098928587166, 1.0296585179700213, 1.026565746119874, 1.0296580430649402, 3.640862843635911, 1.0296580430649402, 1.0296585179700213, 1.0296585179700213, 13.281690918987607, 1.865681517717237, 2.812116244560274, 5.617559882769078, 1.0296585179700213, 1.0278284237494992, 1.0296584229890051, 2.3512556264058957, 1.9657521866721337, 3.6441708424688124, 1.0296573781978267, 3.7920775604705894, 8.344083034752767, 13.341188547243915, 5.914053382848766, 7.727105349592544, 6.595092724056688, 6.926426020733907, 10.955421948518639, 6.8834512900591855, 2.8472865750942513, 3.8673747108889547, 2.826927204305516, 2.5368874743159284, 2.0294614631496577, 1.9657529465202634, 1.9667736125205608, 2.8040031365015246, 3.697047110566643, 1.898631821261425, 1.89941661702156, 1.9017329194032484, 3.7055573222401272, 1.8644571484650523, 3.710900845112887, 1.9025111913612884, 1.902510486085386, 1.8835322168340054, 2.8084675329622346, 7.330660295940685, 2.803532364836829, 1.8984322281811121, 1.892252953364286, 0.996532986207375, 0.9965336033237894, 2.8084805805664246, 0.9964679245054002, 0.9965284019140109, 0.996532986207375, 2.927365589333986, 0.9965293716683764, 2.7701454852231757, 1.9514084986153732, 2.7948719293961837, 0.9912967534312325, 0.9965320164530095, 4.638364751669356, 5.819264616737807, 1.8994118564092206, 6.139653121918295, 6.880318358303021, 3.7144476776248787, 2.8077162378074525, 6.410381740275482, 3.734514892869204, 4.463337841580662, 5.248899531345461, 4.83027561421795, 3.1982668239682277, 4.380253871839868, 2.7879789153663657, 2.960650909217728, 1.957880815569549, 1.9356309477269966, 1.9224315328989872, 3.563975966960749, 3.457897838040273, 3.552092394893681, 1.8395348028097833, 1.8395371080613967, 1.4177957712325797, 1.5987613161009346, 3.3745448795246773, 3.633723659775727, 3.62968946945228, 3.216342377051994, 0.9635483283944778, 0.9631448270317183, 0.9635430592479329, 0.959662168156777, 0.963546517125353, 0.960433851134362, 0.9575975683457345, 3.536377823966804, 0.8563399675401187, 0.9635459408124497, 0.9635478344119892, 0.9635428945871034, 0.9599482663480826, 0.9406921702997078, 4.537096770349448, 1.1470139186375634, 1.1254761998048275, 0.9635451998387168, 0.9635480814032336, 1.7339911629419567, 7.940457624363264, 3.7419917777289315, 5.061856014581955, 4.832162390431247, 6.186919245026325, 2.9741131239492846, 5.46507020885472, 5.7965150046475555, 4.875922981808386, 2.7164327572469436, 8.55580370667421, 5.024955852006289, 2.426791558872301, 2.7155200422688672, 4.401406696489895, 2.3560937730709437, 2.829171087400186, 2.605475892608369, 1.9711282798914989, 2.0547196672882517, 2.5845571763049975, 4.281159049021426, 1.755395162691367, 1.7583175602650154, 2.592189685376601, 1.7628269641272645, 3.387349960021922, 1.6336536200231462, 2.4691298329242435, 0.9233721662425617, 0.9233698062983569, 0.9233717729185276, 0.923371885296823, 0.9233705929464251, 0.9233717729185276, 0.9233717729185276, 0.9233713795944934, 0.9233716043510843, 0.9233684015696635, 1.6035328654888277, 1.7628269641272645, 1.7628271888838554, 3.593243404550644, 2.4444711130628574, 4.439106743301859, 0.9233699748658001, 1.7628269641272645, 1.7553947131781853, 0.9233670530301179, 0.9585314578433407, 1.7628246041830595, 2.9019703461244566, 4.288027160927255, 4.565980939858995, 3.441730491468715, 2.527503839478791, 1.7967495887755647, 1.9311304307133799, 1.8344184562815897, 1.762824716561355, 1.6009422086434013, 1.9000845780507678, 1.2066628466152303, 1.2078018006397997, 3.942470158158041, 1.7345856278125247, 1.7931608207033285, 7.957304061362344, 1.3115608539391468, 2.6049701019872438, 1.8114238025124338, 6.847768625684926, 0.9584189532248608, 0.958333997951903, 0.9584164623896327, 3.56265336923581, 2.6714158004483837, 0.9584175298904447, 0.9584185973912568, 1.813682812147503, 3.977894816771835, 4.262935325265515, 0.9584180636408507, 0.9584185973912568, 0.9584180636408507, 2.2624055330690114, 1.699055108701537, 1.0969205117063918, 0.9584188642664597, 0.9584118365527805, 1.1471420659510128, 0.9584183305160537, 0.9584179746824497, 2.093508223340785, 3.3187443786970667, 3.5264650917078066, 2.847073770736033, 2.898741877539363, 1.015407217200173, 1.0064405662126787, 0.9613168620959331, 0.9584186863496578], \"Total\": [56.0, 21.0, 52.0, 20.0, 66.0, 34.0, 24.0, 19.0, 22.0, 17.0, 15.0, 87.0, 71.0, 15.0, 56.0, 19.0, 50.0, 8.0, 11.0, 15.0, 28.0, 19.0, 16.0, 23.0, 35.0, 8.0, 23.0, 51.0, 10.0, 11.0, 6.587937172939228, 7.447084600143536, 6.462358771661718, 3.6437250994503527, 3.6437230419272693, 3.643686625722109, 23.270554929957612, 3.7315718796736963, 3.7573429411309127, 4.68184504882201, 5.588935466149293, 3.7381636655596977, 23.248197178747287, 6.507930438705734, 7.4727884661152855, 4.606369093010319, 4.693509317064886, 11.904518644670844, 3.7315313368500815, 12.234289415598143, 15.470118393535035, 13.904804240363546, 4.63098752673007, 6.479997877849765, 5.586210990111787, 5.574148600427615, 52.077242279592454, 3.7368204647259318, 5.5390501861834975, 87.79351769866578, 12.7382036543244, 9.16492314652612, 134.87995703840488, 24.627618333596658, 66.66864330558973, 20.543231302571883, 44.269299979986066, 51.3245779218463, 12.050156810827165, 56.78760006197546, 26.633504649874506, 22.76060160626043, 20.89692187824557, 50.523054402563595, 24.14189709034534, 15.28876237665203, 50.5874807456011, 48.076082235342156, 37.96059326691158, 35.93013864108156, 71.94633521652968, 19.659273398185107, 56.26977984970157, 6.443447302438875, 4.659237177253126, 3.7641136499233068, 9.446107238324801, 3.7535395849437605, 5.640450719077934, 6.333023748721884, 10.310688338164246, 9.290134887599601, 4.6847773122498895, 4.685320652774362, 4.6722767277932, 20.442178793646015, 5.643314852893304, 7.497904859185718, 16.730764971848238, 4.518911848572709, 12.234289415598143, 5.587581230678945, 51.3245779218463, 4.669003793937591, 6.560889507805737, 18.590600035214578, 3.6400760255971116, 3.667883868693329, 3.716643559811522, 3.7211842359561116, 7.179866075605167, 56.78760006197546, 48.076082235342156, 17.614514308824482, 11.990817579502444, 21.273253895595424, 134.87995703840488, 87.79351769866578, 71.94633521652968, 44.269299979986066, 52.077242279592454, 50.523054402563595, 35.93013864108156, 50.5874807456011, 12.180868686942082, 13.095620189773728, 23.248197178747287, 25.8938129554938, 8.31763602589793, 4.564871404961936, 6.480098692336533, 3.605419333173114, 3.605548600316964, 5.586210990111787, 15.747882241776368, 15.670642263390253, 9.080256977962428, 4.51177521411954, 4.648497994699683, 4.605591451539268, 4.6722767277932, 11.027631039128845, 17.55642759015492, 5.522086326021822, 7.4727884661152855, 13.034880311632524, 3.6039681442794964, 3.615921881940501, 3.62744320718607, 3.648427956999687, 3.6508601457271674, 3.661557997177506, 3.667883868693329, 3.690497881377226, 3.711570242542309, 3.711929312900712, 3.7188163833010837, 3.7315718796736963, 50.523054402563595, 7.406291300967181, 7.373308326890512, 35.93013864108156, 134.87995703840488, 66.66864330558973, 12.7382036543244, 23.248197178747287, 87.79351769866578, 71.94633521652968, 48.076082235342156, 23.270554929957612, 44.269299979986066, 19.27779969139979, 50.5874807456011, 26.633504649874506, 28.303400157705124, 19.195284138806226, 56.26977984970157, 37.96059326691158, 51.3245779218463, 19.27779969139979, 3.690497881377226, 17.55642759015492, 5.460799303475332, 3.716643559811522, 3.6982462639966736, 3.612692057994389, 4.66234983348072, 4.662600233677029, 34.824279274858064, 15.747882241776368, 3.7211842359561116, 5.369152760882981, 7.406291300967181, 10.087144658543355, 11.027631039128845, 5.529900191617985, 8.328812896099114, 66.66864330558973, 6.411190299791609, 71.94633521652968, 6.507930438705734, 6.398950183697947, 18.590600035214578, 10.310688338164246, 10.071423465135066, 3.6039681442794964, 7.282765190226419, 3.649372571134624, 3.649539166936481, 37.96059326691158, 28.303400157705124, 56.78760006197546, 134.87995703840488, 48.076082235342156, 51.3245779218463, 50.5874807456011, 50.523054402563595, 35.93013864108156, 20.442178793646015, 12.180868686942082, 44.269299979986066, 12.935076048703518, 20.543231302571883, 87.79351769866578, 25.8938129554938, 4.658948458036397, 6.451924762792057, 3.618935465838902, 3.725482667657376, 7.471167304699097, 5.587739663777764, 7.413483792976032, 5.5799558577935295, 4.5309845283341605, 4.612149313239655, 4.659373549417901, 16.730764971848238, 4.685320652774362, 4.67842401915786, 9.16492314652612, 4.6847773122498895, 6.373453174502163, 13.095620189773728, 3.7315313368500815, 7.497904859185718, 6.398950183697947, 5.44867911875127, 5.518444149408883, 5.574148600427615, 25.8938129554938, 5.576803056839489, 9.14867407272812, 4.669003793937591, 20.543231302571883, 3.6224390739916528, 21.0977654357191, 34.824279274858064, 15.28876237665203, 24.14189709034534, 56.26977984970157, 37.96059326691158, 28.303400157705124, 26.633504649874506, 13.904804240363546, 56.78760006197546, 44.269299979986066, 11.632443519609803, 12.050156810827165, 87.79351769866578, 50.523054402563595, 71.94633521652968, 134.87995703840488, 50.5874807456011, 22.76060160626043, 20.442178793646015, 12.935076048703518, 18.590600035214578, 3.7173653993150073, 3.711570242542309, 3.711929312900712, 4.58687466758194, 4.623226092719848, 6.560889507805737, 4.66134638200971, 5.304333857863198, 12.180868686942082, 8.328812896099114, 12.935076048703518, 3.6400760255971116, 3.649539166936481, 3.649372571134624, 3.6611242266193327, 13.034880311632524, 3.69975484607813, 3.7381636655596977, 3.7535395849437605, 52.077242279592454, 7.406291300967181, 11.873161045625386, 24.14189709034534, 4.648497994699683, 4.659237177253126, 4.693509317064886, 11.990817579502444, 10.087144658543355, 19.195284138806226, 5.45261565240697, 21.0977654357191, 50.5874807456011, 87.79351769866578, 37.96059326691158, 56.78760006197546, 51.3245779218463, 66.66864330558973, 134.87995703840488, 71.94633521652968, 24.627618333596658, 44.269299979986066, 26.633504649874506, 50.523054402563595, 13.666999605843737, 13.095620189773728, 35.93013864108156, 4.5569025405811745, 6.398011683574595, 3.5630240996597724, 3.6508601457271674, 3.6571590157115197, 7.1419174578348015, 3.6611242266193327, 8.118558449032188, 4.602350087291175, 4.6023596515175305, 4.5944994039499925, 7.20658627919582, 19.195284138806226, 7.373308326890512, 5.421030342623548, 5.45261565240697, 3.528181416882682, 3.5694034130888097, 10.071423465135066, 3.615921881940501, 3.6224390739916528, 3.6895616145756853, 10.84071032786983, 3.69975484607813, 10.781345160948751, 8.001614582048282, 11.904518644670844, 4.36873754478974, 4.466421040839045, 20.89692187824557, 28.303400157705124, 8.961692002642268, 35.93013864108156, 48.076082235342156, 23.270554929957612, 15.670642263390253, 52.077242279592454, 24.627618333596658, 37.96059326691158, 50.5874807456011, 51.3245779218463, 25.8938129554938, 87.79351769866578, 26.633504649874506, 134.87995703840488, 66.66864330558973, 71.94633521652968, 21.0977654357191, 5.256428721000718, 5.346217560308469, 6.216529799900249, 3.5694034130888097, 4.5309845283341605, 3.62744320718607, 4.518911848572709, 9.991164322924087, 11.632443519609803, 11.873161045625386, 10.84071032786983, 3.648427956999687, 3.649372571134624, 3.6595755092045787, 3.649539166936481, 3.667883868693329, 3.667765269240089, 3.661557997177506, 13.666999605843737, 3.676047070155715, 4.466421040839045, 4.573451538749871, 4.58687466758194, 4.606369093010319, 4.62254603213192, 22.76060160626043, 6.398950183697947, 6.333023748721884, 5.518444149408883, 5.522086326021822, 10.071423465135066, 50.5874807456011, 23.248197178747287, 35.93013864108156, 34.824279274858064, 52.077242279592454, 20.442178793646015, 48.076082235342156, 56.26977984970157, 44.269299979986066, 19.27779969139979, 134.87995703840488, 71.94633521652968, 19.659273398185107, 25.8938129554938, 66.66864330558973, 21.273253895595424, 50.523054402563595, 87.79351769866578, 21.0977654357191, 56.78760006197546, 5.304333857863198, 8.824385657606406, 4.36873754478974, 4.464466534087622, 7.179866075605167, 5.369152760882981, 10.781345160948751, 5.412164999950035, 8.961692002642268, 3.528181416882682, 3.5630240996597724, 3.6039681442794964, 3.605419333173114, 3.605548600316964, 3.618935465838902, 3.6400760255971116, 3.643686625722109, 3.6437230419272693, 3.6437250994503527, 6.333023748721884, 7.1419174578348015, 7.192369162332945, 15.28876237665203, 11.632443519609803, 21.273253895595424, 4.466421040839045, 9.080256977962428, 9.16492314652612, 5.256428721000718, 5.460799303475332, 12.7382036543244, 26.633504649874506, 50.523054402563595, 71.94633521652968, 44.269299979986066, 24.627618333596658, 16.730764971848238, 21.0977654357191, 34.824279274858064, 37.96059326691158, 28.303400157705124, 134.87995703840488, 13.904804240363546, 66.66864330558973, 8.001614582048282, 3.6077272936364926, 4.51177521411954, 21.273253895595424, 3.612692057994389, 7.282765190226419, 5.44867911875127, 20.89692187824557, 3.528181416882682, 3.615921881940501, 3.6224390739916528, 15.470118393535035, 11.904518644670844, 4.36873754478974, 4.564871404961936, 10.087144658543355, 22.76060160626043, 24.627618333596658, 6.216529799900249, 6.373453174502163, 6.451924762792057, 17.614514308824482, 15.28876237665203, 9.991164322924087, 8.961692002642268, 9.290134887599601, 12.7382036543244, 10.84071032786983, 11.990817579502444, 28.303400157705124, 50.523054402563595, 56.78760006197546, 87.79351769866578, 134.87995703840488, 19.659273398185107, 51.3245779218463, 19.195284138806226, 24.14189709034534], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.18179988861084, -4.304699897766113, -4.550300121307373, -5.217800140380859, -5.217800140380859, -5.217800140380859, -3.3808000087738037, -5.217800140380859, -5.217899799346924, -5.035600185394287, -4.863900184631348, -5.302000045776367, -3.552000045776367, -4.835599899291992, -4.706500053405762, -5.217800140380859, -5.217899799346924, -4.328800201416016, -5.495800018310547, -4.340400218963623, -4.112299919128418, -4.2281999588012695, -5.340400218963623, -5.011099815368652, -5.217899799346924, -5.231299877166748, -3.027100086212158, -5.686600208282471, -5.299099922180176, -2.569700002670288, -4.509200096130371, -4.83050012588501, -2.4031999111175537, -3.9753000736236572, -3.247299909591675, -4.216599941253662, -3.8259999752044678, -3.8022000789642334, -4.672399997711182, -3.84060001373291, -4.275599956512451, -4.369200229644775, -4.437699794769287, -4.059100151062012, -4.411099910736084, -4.621799945831299, -4.3165998458862305, -4.37470006942749, -4.447199821472168, -4.529300212860107, -4.4644999504089355, -3.127000093460083, -2.2936999797821045, -4.525599956512451, -4.867099761962891, -5.1097002029418945, -4.193299770355225, -5.117199897766113, -4.763299942016602, -4.718900203704834, -4.287399768829346, -4.397299766540527, -5.107100009918213, -5.107999801635742, -5.111999988555908, -3.655400037765503, -4.993500232696533, -4.74560022354126, -3.979099988937378, -5.30019998550415, -4.30649995803833, -5.10290002822876, -2.8975000381469727, -5.375199794769287, -5.072999954223633, -4.094799995422363, -5.755099773406982, -5.753799915313721, -5.753799915313721, -5.753799915313721, -5.10129976272583, -3.084399938583374, -3.291800022125244, -4.293300151824951, -4.712299823760986, -4.240300178527832, -2.7455999851226807, -3.283099889755249, -3.467900037765503, -3.8143999576568604, -3.88319993019104, -3.9746999740600586, -4.226799964904785, -4.170899868011475, -4.784200191497803, -4.785699844360352, -4.720699787139893, -4.753799915313721, -3.9367001056671143, -4.616099834442139, -4.331999778747559, -5.000699996948242, -5.000999927520752, -4.610499858856201, -3.5782999992370605, -3.708899974822998, -4.292099952697754, -4.999800205230713, -4.99970006942749, -5.009799957275391, -4.99970006942749, -4.155099868774414, -3.7112998962402344, -4.99970006942749, -4.769499778747559, -4.3379998207092285, -5.646399974822998, -5.646399974822998, -5.646399974822998, -5.646399974822998, -5.646399974822998, -5.646399974822998, -5.646399974822998, -5.647299766540527, -5.646399974822998, -5.646599769592285, -5.646399974822998, -5.64870023727417, -3.1257998943328857, -4.991799831390381, -5.000899791717529, -3.5146000385284424, -2.3645999431610107, -3.0208001136779785, -4.611000061035156, -4.149600028991699, -3.0894999504089355, -3.3376998901367188, -3.6572999954223633, -4.307400226593018, -3.971100091934204, -4.464099884033203, -4.00570011138916, -4.313300132751465, -4.326499938964844, -4.643899917602539, -4.428299903869629, -4.690899848937988, -4.712399959564209, -3.327500104904175, -4.991799831390381, -3.4403998851776123, -4.613999843597412, -5.016900062561035, -5.026700019836426, -5.21589994430542, -4.989500045776367, -4.9899001121521, -3.0111000537872314, -3.8698999881744385, -5.316500186920166, -4.960999965667725, -4.650100231170654, -4.3420000076293945, -4.292099952697754, -4.9899001121521, -4.600399971008301, -2.5694000720977783, -4.956699848175049, -2.5624001026153564, -4.9899001121521, -5.01230001449585, -3.952899932861328, -4.543000221252441, -4.571700096130371, -5.636499881744385, -4.942699909210205, -5.636499881744385, -5.636499881744385, -3.4860999584198, -3.828000068664551, -3.420099973678589, -2.770400047302246, -3.7118000984191895, -3.7121999263763428, -3.822499990463257, -3.8268001079559326, -4.026700019836426, -4.385799884796143, -4.678400039672852, -4.457300186157227, -4.791600227355957, -4.733399868011475, -4.522299766540527, -4.711400032043457, -4.564700126647949, -4.2895002365112305, -4.970799922943115, -4.955999851226807, -4.2881999015808105, -4.58650016784668, -4.322700023651123, -4.711299896240234, -4.952499866485596, -4.94980001449585, -4.94980001449585, -3.6744000911712646, -4.94890022277832, -4.952499866485596, -4.281400203704834, -4.959799766540527, -4.659200191497803, -3.9540998935699463, -5.215799808502197, -4.53439998626709, -4.730400085449219, -4.94980001449585, -4.957399845123291, -4.959799766540527, -3.43179988861084, -5.037199974060059, -4.542600154876709, -5.233799934387207, -3.846299886703491, -5.596399784088135, -3.839400053024292, -3.3733999729156494, -4.162300109863281, -3.7802999019622803, -3.2014000415802, -3.5799999237060547, -3.7867000102996826, -3.88070011138916, -4.324399948120117, -3.5446999073028564, -3.7049999237060547, -4.4405999183654785, -4.438700199127197, -3.5734000205993652, -3.8320000171661377, -3.732100009918213, -3.523099899291992, -3.9079999923706055, -4.23829984664917, -4.331999778747559, -4.449399948120117, -4.497499942779541, -4.642099857330322, -4.678999900817871, -4.6930999755859375, -4.642099857330322, -4.64169979095459, -4.293600082397461, -4.758900165557861, -4.659200191497803, -3.840399980545044, -4.275700092315674, -3.973599910736084, -5.287499904632568, -5.288700103759766, -5.291800022125244, -5.288700103759766, -4.0258002281188965, -5.288700103759766, -5.288700103759766, -5.288700103759766, -2.731600046157837, -4.694300174713135, -4.283999919891357, -3.592099905014038, -5.288700103759766, -5.290500164031982, -5.288700103759766, -4.4629998207092285, -4.642099857330322, -4.024799823760986, -5.288700103759766, -3.985100030899048, -3.1963999271392822, -2.727099895477295, -3.540600061416626, -3.273200035095215, -3.4316000938415527, -3.3826000690460205, -2.924099922180176, -3.3889000415802, -4.271599769592285, -3.965399980545044, -4.278800010681152, -4.38700008392334, -4.610199928283691, -4.642099857330322, -4.641600131988525, -4.212399959564209, -3.9358999729156494, -4.60230016708374, -4.601900100708008, -4.6006999015808105, -3.9335999488830566, -4.620500087738037, -3.9321999549865723, -4.600299835205078, -4.600299835205078, -4.610300064086914, -4.2108001708984375, -3.2513999938964844, -4.212600231170654, -4.602399826049805, -4.6057000160217285, -5.2469000816345215, -5.2469000816345215, -4.2108001708984375, -5.247000217437744, -5.2469000816345215, -5.2469000816345215, -4.169300079345703, -5.2469000816345215, -4.2245001792907715, -4.574900150299072, -4.215700149536133, -5.252200126647949, -5.2469000816345215, -3.7091000080108643, -3.482300043106079, -4.601900100708008, -3.4286999702453613, -3.3148000240325928, -3.9312000274658203, -4.211100101470947, -3.385499954223633, -3.925800085067749, -3.747499942779541, -3.585400104522705, -3.6684999465942383, -4.0808000564575195, -3.7662999629974365, -4.218100070953369, -4.1579999923706055, -4.571599960327148, -4.583000183105469, -4.589900016784668, -3.9042000770568848, -3.9344000816345215, -3.9075000286102295, -4.565499782562256, -4.565499782562256, -4.825900077819824, -4.7058000564575195, -3.9588000774383545, -3.8847999572753906, -3.8859000205993652, -4.006800174713135, -5.212200164794922, -5.212600231170654, -5.212200164794922, -5.21619987487793, -5.212200164794922, -5.215400218963623, -5.218400001525879, -3.911900043487549, -5.330100059509277, -5.212200164794922, -5.212200164794922, -5.212200164794922, -5.21589994430542, -5.236199855804443, -3.6628000736236572, -5.037899971008301, -5.05679988861084, -5.212200164794922, -5.212200164794922, -4.624599933624268, -3.103100061416626, -3.8554000854492188, -3.553299903869629, -3.5996999740600586, -3.35260009765625, -4.085100173950195, -3.4767000675201416, -3.417799949645996, -3.5906999111175537, -4.1757001876831055, -3.02839994430542, -3.5606000423431396, -4.28849983215332, -4.17609977722168, -3.6930999755859375, -4.317999839782715, -4.1350998878479, -4.217400074005127, -4.496399879455566, -4.454899787902832, -3.8434998989105225, -3.3387999534606934, -4.230299949645996, -4.228700160980225, -3.8405001163482666, -4.226099967956543, -3.572999954223633, -4.302199840545654, -3.88919997215271, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.872700214385986, -4.320799827575684, -4.226099967956543, -4.226099967956543, -3.5139999389648438, -3.899199962615967, -3.3025999069213867, -4.872700214385986, -4.226099967956543, -4.230299949645996, -4.872700214385986, -4.835400104522705, -4.226099967956543, -3.72760009765625, -3.3371999263763428, -3.274399995803833, -3.556999921798706, -3.865799903869629, -4.206999778747559, -4.134900093078613, -4.186299800872803, -4.226099967956543, -4.322400093078613, -4.151100158691406, -4.605199813842773, -4.6041998863220215, -3.1875, -4.008500099182129, -3.9753000736236572, -2.4851999282836914, -4.288099765777588, -3.601900100708008, -3.9651999473571777, -2.6354000568389893, -4.601799964904785, -4.601900100708008, -4.601799964904785, -3.288800001144409, -3.57669997215271, -4.601799964904785, -4.601799964904785, -3.9639999866485596, -3.1786000728607178, -3.1094000339508057, -4.601799964904785, -4.601799964904785, -4.601799964904785, -3.7428998947143555, -4.029200077056885, -4.466800212860107, -4.601799964904785, -4.601799964904785, -4.421999931335449, -4.601799964904785, -4.601799964904785, -3.820499897003174, -3.3596999645233154, -3.2990000247955322, -3.513000011444092, -3.494999885559082, -4.544000148773193, -4.5528998374938965, -4.598800182342529, -4.601799964904785], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.6888, 1.4433, 1.3394, 1.2449, 1.2449, 1.2449, 1.2277, 1.2211, 1.2142, 1.1765, 1.1711, 1.1352, 1.0575, 1.0471, 1.038, 1.0105, 0.9917, 0.9501, 0.9432, 0.9111, 0.9045, 0.8953, 0.8826, 0.8759, 0.8176, 0.8063, 0.7759, 0.7509, 0.7448, 0.7111, 0.702, 0.7099, 0.4482, 0.5766, 0.3087, 0.5166, 0.1395, 0.0154, 0.5943, -0.1241, 0.198, 0.2616, 0.2784, -0.2258, 0.1607, 0.4068, -0.4845, -0.4918, -0.328, -0.3552, -0.9846, 1.6502, 1.4319, 1.3671, 1.3498, 1.3205, 1.3169, 1.3159, 1.2625, 1.1911, 1.1352, 1.1295, 1.1043, 1.1034, 1.1021, 1.0828, 1.0318, 0.9956, 0.9594, 0.9473, 0.945, 0.9323, 0.9201, 0.8396, 0.8017, 0.7384, 0.7087, 0.7024, 0.6892, 0.6879, 0.6832, 0.632, 0.5912, 0.5937, 0.5593, 0.458, 0.1058, -0.0023, 0.0119, 0.1511, -0.0801, -0.1413, -0.0526, -0.3388, 0.4717, 0.3978, -0.1112, -0.2521, 1.7007, 1.6213, 1.555, 1.4726, 1.4723, 1.425, 1.4208, 1.295, 1.2576, 1.2493, 1.2195, 1.2187, 1.2144, 1.2002, 1.179, 1.0473, 0.975, 0.8501, 0.8273, 0.824, 0.8208, 0.8151, 0.8144, 0.8115, 0.8098, 0.8027, 0.7979, 0.7976, 0.796, 0.7903, 0.7076, 0.7617, 0.757, 0.6596, 0.4868, 0.5353, 0.6002, 0.4599, 0.1913, 0.1421, 0.2257, 0.3011, -0.0057, 0.3327, -0.1736, 0.1603, 0.0863, 0.1572, -0.7027, -0.5717, -0.8948, 1.4693, 1.4582, 1.4499, 1.4442, 1.426, 1.4212, 1.2554, 1.2267, 1.2263, 1.1944, 1.1292, 1.1252, 1.1141, 1.1033, 1.1025, 1.0632, 1.0557, 1.0357, 0.9866, 0.941, 0.9175, 0.8929, 0.8873, 0.8802, 0.8795, 0.8744, 0.8372, 0.8276, 0.8247, 0.8247, 0.6331, 0.5848, 0.2964, 0.081, 0.1712, 0.1054, 0.0096, 0.0065, 0.1475, 0.3524, 0.5775, -0.4918, 0.4042, -0.0001, -1.2415, -0.2096, 1.6523, 1.6019, 1.4988, 1.4846, 1.4566, 1.4486, 1.4297, 1.3253, 1.2923, 1.2773, 1.2671, 1.2641, 1.2624, 1.2603, 1.259, 1.2516, 1.2445, 1.2294, 1.2232, 1.2068, 1.1692, 1.1106, 1.0903, 1.0778, 1.07, 1.0, 0.9996, 0.981, 0.887, 0.8722, 0.8672, 0.832, 0.8664, 0.7915, 0.5242, 0.5392, 0.6261, 0.5929, 0.7991, 0.1717, 0.2604, 0.8613, 0.828, -0.2926, 0.0013, -0.2522, -0.6718, -0.0759, 0.3925, 0.4062, 0.7464, 0.3356, 1.8007, 1.7654, 1.7511, 1.5905, 1.583, 1.581, 1.4575, 1.428, 1.4155, 1.3604, 1.2222, 1.1762, 1.1724, 1.1695, 1.1692, 1.1624, 1.1588, 1.1484, 1.1443, 1.0715, 1.0591, 0.9975, 0.9797, 0.9305, 0.9264, 0.9208, 0.8086, 0.8024, 0.7763, 0.7709, 0.7215, 0.6356, 0.5537, 0.5786, 0.4432, 0.3859, 0.1734, -0.0728, 0.091, 0.2803, 0.0001, 0.1948, -0.5537, 0.5306, 0.5414, -0.4674, 2.0267, 1.9639, 1.8828, 1.8589, 1.8584, 1.8562, 1.8375, 1.7294, 1.6289, 1.6289, 1.6206, 1.57, 1.5497, 1.5453, 1.4631, 1.454, 1.2481, 1.2365, 1.2353, 1.2234, 1.2217, 1.2033, 1.2031, 1.2006, 1.1534, 1.1012, 1.0632, 1.0291, 1.0123, 1.0071, 0.9305, 0.9609, 0.7455, 0.5682, 0.6774, 0.7929, 0.4175, 0.6261, 0.3717, 0.2466, 0.1491, 0.4209, -0.4856, 0.2555, -1.3067, -1.0155, -1.1032, 0.1167, 2.1922, 2.145, 2.0211, 1.9178, 1.6793, 1.6413, 1.5417, 1.4953, 1.4172, 1.3956, 1.3657, 1.2493, 1.2486, 1.2462, 1.245, 1.244, 1.2408, 1.2395, 1.2288, 1.1238, 1.047, 1.0233, 1.0204, 1.0124, 0.9886, 0.968, 0.8618, 0.8532, 0.8355, 0.8348, 0.8215, 0.729, 0.7541, 0.6209, 0.6057, 0.4504, 0.6531, 0.4063, 0.3078, 0.3747, 0.6211, -0.177, -0.0808, 0.4888, 0.3257, -0.1371, 0.3803, -0.3017, -0.9366, 0.2102, -0.7384, 2.2438, 2.2395, 2.051, 2.031, 1.944, 1.849, 1.805, 1.7649, 1.6737, 1.6222, 1.6124, 1.601, 1.6006, 1.6005, 1.5968, 1.591, 1.59, 1.59, 1.59, 1.5892, 1.5637, 1.5566, 1.5147, 1.4028, 1.3957, 1.3864, 1.3236, 1.3101, 1.2236, 1.2228, 0.9851, 0.746, 0.4961, 0.2055, 0.4084, 0.6861, 0.7315, 0.5717, 0.0192, -0.1069, 0.0904, -1.2997, 0.5184, -1.0482, 2.4886, 2.4641, 2.2737, 2.2131, 2.1832, 2.1684, 2.0952, 2.0808, 1.8932, 1.8685, 1.8668, 1.728, 1.7021, 1.6795, 1.6356, 1.4805, 1.4522, 1.4425, 1.3268, 1.3018, 1.2896, 1.1442, 0.9994, 0.9873, 0.961, 0.925, 0.7891, 0.7707, 0.6698, 0.5923, 0.4736, 0.4174, -0.2322, -0.6437, 0.2332, -0.7353, 0.2023, -0.03]}, \"token.table\": {\"Topic\": [1, 2, 4, 1, 4, 7, 10, 2, 4, 7, 8, 3, 5, 8, 2, 3, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 8, 9, 10, 4, 6, 7, 1, 2, 4, 8, 10, 1, 2, 5, 7, 9, 10, 1, 3, 9, 6, 7, 1, 5, 7, 1, 6, 1, 5, 7, 8, 1, 2, 3, 6, 8, 5, 7, 10, 2, 3, 8, 2, 4, 6, 8, 1, 3, 5, 8, 3, 5, 6, 7, 9, 10, 2, 3, 8, 9, 10, 2, 4, 5, 8, 9, 2, 6, 5, 7, 8, 9, 1, 3, 7, 8, 10, 2, 3, 6, 7, 8, 1, 2, 5, 9, 2, 3, 4, 6, 1, 4, 7, 3, 4, 9, 2, 6, 9, 1, 2, 1, 4, 8, 1, 2, 5, 1, 2, 4, 1, 5, 9, 10, 2, 7, 8, 10, 3, 6, 1, 2, 3, 5, 6, 7, 9, 10, 5, 6, 10, 1, 3, 4, 5, 7, 8, 9, 4, 5, 1, 6, 7, 8, 1, 2, 3, 6, 7, 9, 1, 3, 7, 8, 9, 1, 4, 7, 8, 10, 1, 2, 4, 6, 1, 5, 7, 3, 4, 8, 1, 6, 7, 2, 5, 7, 2, 5, 2, 4, 7, 7, 9, 10, 1, 4, 9, 2, 8, 9, 3, 6, 7, 5, 8, 1, 2, 5, 2, 6, 7, 9, 1, 3, 2, 3, 6, 7, 1, 5, 1, 2, 3, 6, 7, 9, 10, 4, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 1, 1, 5, 3, 8, 3, 4, 5, 7, 9, 1, 9, 3, 7, 3, 9, 3, 6, 8, 1, 2, 3, 5, 6, 7, 8, 9, 3, 4, 6, 10, 2, 3, 2, 3, 4, 5, 6, 1, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 4, 1, 2, 6, 4, 5, 8, 1, 5, 8, 9, 10, 2, 3, 5, 1, 2, 3, 2, 4, 7, 1, 2, 5, 6, 7, 8, 1, 2, 4, 1, 2, 5, 7, 9, 2, 7, 9, 1, 3, 5, 5, 6, 1, 5, 9, 2, 8, 9, 3, 7, 10, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 9, 3, 9, 1, 9, 1, 2, 4, 8, 10, 1, 3, 4, 5, 1, 2, 3, 4, 5, 7, 4, 6, 8, 3, 7, 1, 3, 5, 8, 1, 3, 5, 9, 3, 4, 5, 8, 1, 2, 3, 5, 7, 1, 2, 3, 5, 7, 8, 10, 1, 2, 5, 7, 9, 1, 2, 3, 4, 5, 6, 7, 8, 2, 3, 5, 2, 3, 8, 1, 2, 3, 4, 6, 7, 8, 9, 10, 3, 8, 10, 2, 3, 4, 5, 8, 1, 3, 4, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 7, 1, 3, 4, 5, 6, 8, 9, 3, 4, 6, 1, 2, 3, 4, 5, 6, 7, 8, 10, 4, 10, 1, 3, 8, 6, 9, 1, 2, 3, 4, 5, 6, 7, 10, 2, 3, 8, 3, 4, 1, 3, 6, 2, 3, 4, 5, 8, 1, 2, 5, 8, 1, 3, 6, 7, 9, 1, 2, 3, 8, 9, 10, 3, 4, 5, 9, 2, 3, 5, 7, 1, 5, 1, 3, 2, 6, 7, 9, 3, 5, 6, 8, 1, 2, 3, 4, 6, 7, 8, 1, 3, 6, 3, 4, 6, 2, 5, 1, 4, 7, 1, 4, 8, 5, 7, 3, 4, 6, 7, 8, 3, 4, 6, 9, 4, 6, 7, 8, 9, 3, 4, 5, 6, 9, 2, 5, 6, 3, 6, 7, 9, 10, 8, 9, 2, 5, 8, 10, 4, 6, 7, 9, 3, 10, 2, 3, 6, 7, 1, 5, 8, 1, 2, 3, 4, 5, 6, 7, 9, 1, 2, 4, 10, 4, 5, 10, 7, 9, 1, 2, 3, 1, 5, 6, 3, 4, 5, 7, 5, 6, 7, 1, 3, 5, 3, 5, 6, 7, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 9, 3, 4, 9, 5, 7, 8, 9, 1, 2, 3, 4, 7, 10, 1, 4, 7, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 2, 4, 1, 2, 4, 1, 3, 5, 8, 4, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 7, 9, 1, 10, 2, 4, 5, 1, 3, 4, 5, 6, 7, 8, 9, 2, 4, 5, 6, 9, 10, 7, 8, 1, 4, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 10, 5, 9], \"Freq\": [0.40868740554929456, 0.32694992443943566, 0.16347496221971783, 0.4200085823913839, 0.08400171647827678, 0.2520051494348303, 0.2520051494348303, 0.19858166096615207, 0.29787249144922806, 0.29787249144922806, 0.19858166096615207, 0.27409065268274, 0.27409065268274, 0.27409065268274, 0.3227437516075175, 0.053790625267919584, 0.26895312633959795, 0.16137187580375875, 0.10758125053583917, 0.053790625267919584, 0.11875766560335975, 0.11875766560335975, 0.25730827547394614, 0.11875766560335975, 0.11875766560335975, 0.059378832801679875, 0.059378832801679875, 0.0791717770689065, 0.059378832801679875, 0.15629855796719896, 0.15629855796719896, 0.6251942318687959, 0.2001768698179779, 0.10008843490898894, 0.2001768698179779, 0.30026530472696683, 0.10008843490898894, 0.1115860709902951, 0.1115860709902951, 0.1115860709902951, 0.2231721419805902, 0.2231721419805902, 0.1115860709902951, 0.22399092755308656, 0.22399092755308656, 0.4479818551061731, 0.2731401444204465, 0.546280288840893, 0.5367748506258855, 0.17892495020862847, 0.17892495020862847, 0.5350220533216138, 0.2675110266608069, 0.21865324067112446, 0.21865324067112446, 0.21865324067112446, 0.21865324067112446, 0.47315496833689225, 0.12904226409187972, 0.21507044015313284, 0.04301408803062657, 0.17205635212250628, 0.2760570929073145, 0.2760570929073145, 0.2760570929073145, 0.1543186373353566, 0.6172745493414264, 0.1543186373353566, 0.12006512962589908, 0.3601953888776972, 0.3601953888776972, 0.12006512962589908, 0.361072734995029, 0.1805363674975145, 0.1805363674975145, 0.1805363674975145, 0.1562883872052218, 0.1562883872052218, 0.20838451627362908, 0.3646729034788509, 0.05209612906840727, 0.05209612906840727, 0.2350369165215124, 0.04700738330430248, 0.09401476660860496, 0.18802953321720992, 0.37605906643441983, 0.08596646081403402, 0.08596646081403402, 0.25789938244210203, 0.34386584325613606, 0.17193292162806803, 0.6438822248943041, 0.21462740829810137, 0.24634915330792745, 0.4926983066158549, 0.12317457665396372, 0.12317457665396372, 0.1844897556997074, 0.0922448778498537, 0.2767346335495611, 0.2767346335495611, 0.0922448778498537, 0.1914407814036186, 0.44669515660844333, 0.06381359380120619, 0.1914407814036186, 0.12762718760241237, 0.059770130157385776, 0.35862078094431465, 0.41839091110170046, 0.11954026031477155, 0.48493367620208944, 0.09698673524041788, 0.29096020572125364, 0.09698673524041788, 0.431873329059082, 0.215936664529541, 0.215936664529541, 0.18624912430979995, 0.3724982486195999, 0.3724982486195999, 0.27471953689098066, 0.27471953689098066, 0.27471953689098066, 0.26566679250515585, 0.5313335850103117, 0.27325573621443144, 0.27325573621443144, 0.27325573621443144, 0.2137471926240677, 0.2137471926240677, 0.4274943852481354, 0.31759114355894486, 0.5293185725982414, 0.10586371451964828, 0.15690081540108136, 0.47070244620324414, 0.15690081540108136, 0.15690081540108136, 0.12497477724602132, 0.24994955449204265, 0.12497477724602132, 0.4998991089840853, 0.2694277447690257, 0.5388554895380514, 0.16568706199976524, 0.08284353099988262, 0.08284353099988262, 0.24853059299964786, 0.24853059299964786, 0.08284353099988262, 0.04142176549994131, 0.04142176549994131, 0.6199700317442959, 0.15499250793607397, 0.15499250793607397, 0.05743119575324367, 0.028715597876621837, 0.4020183702727057, 0.25844038088959653, 0.028715597876621837, 0.14357798938310917, 0.05743119575324367, 0.5407968689025623, 0.27039843445128114, 0.25267070736022207, 0.25267070736022207, 0.16844713824014804, 0.3368942764802961, 0.5156731344017784, 0.0429727612001482, 0.1718910448005928, 0.0429727612001482, 0.1718910448005928, 0.0429727612001482, 0.27807248972621873, 0.13903624486310937, 0.13903624486310937, 0.13903624486310937, 0.27807248972621873, 0.3878444784564416, 0.12928149281881388, 0.12928149281881388, 0.12928149281881388, 0.25856298563762775, 0.16419190218707508, 0.24628785328061262, 0.24628785328061262, 0.32838380437415016, 0.35879918950251377, 0.35879918950251377, 0.17939959475125689, 0.39871437193323855, 0.5126327639141638, 0.05695919599046265, 0.2702881789748948, 0.2702881789748948, 0.2702881789748948, 0.17921288724951095, 0.3584257744990219, 0.17921288724951095, 0.42691463578649574, 0.42691463578649574, 0.21727984679994994, 0.21727984679994994, 0.4345596935998999, 0.22889907890956365, 0.4577981578191273, 0.22889907890956365, 0.6189690392214866, 0.15474225980537165, 0.15474225980537165, 0.44258442452947977, 0.44258442452947977, 0.22129221226473988, 0.7213588069156068, 0.12022646781926781, 0.12022646781926781, 0.4414051708835365, 0.4414051708835365, 0.3544016331065789, 0.3544016331065789, 0.17720081655328945, 0.27855672779125296, 0.13927836389562648, 0.13927836389562648, 0.4178350916868794, 0.3580244289985147, 0.5370366434977721, 0.13562432976700464, 0.2712486595340093, 0.13562432976700464, 0.4068729893010139, 0.21464070895119308, 0.6439221268535792, 0.2842337373099004, 0.08120963923140012, 0.08120963923140012, 0.12181445884710018, 0.16241927846280024, 0.12181445884710018, 0.16241927846280024, 0.27401970626668315, 0.27401970626668315, 0.27401970626668315, 0.09883868353011, 0.09883868353011, 0.09883868353011, 0.11860642023613199, 0.11860642023613199, 0.158141893648176, 0.09883868353011, 0.158141893648176, 0.019767736706022, 0.019767736706022, 0.5328304004099017, 0.26641520020495085, 0.9107554978887392, 0.5359729878855235, 0.5359729878855235, 0.1870481305183363, 0.561144391555009, 0.10930545694932617, 0.10930545694932617, 0.3279163708479785, 0.21861091389865234, 0.10930545694932617, 0.5488888281670028, 0.2744444140835014, 0.2194473090206715, 0.6583419270620144, 0.5547006077866153, 0.27735030389330767, 0.21801336911766317, 0.43602673823532634, 0.21801336911766317, 0.18773346075667738, 0.07509338430267094, 0.15018676860534189, 0.22528015290801284, 0.11264007645400642, 0.11264007645400642, 0.07509338430267094, 0.11264007645400642, 0.19827216399698308, 0.39654432799396616, 0.19827216399698308, 0.19827216399698308, 0.4280568374948621, 0.4280568374948621, 0.22908422484203364, 0.07636140828067789, 0.07636140828067789, 0.3818070414033894, 0.15272281656135578, 0.27203133717154515, 0.27203133717154515, 0.27203133717154515, 0.20999377377199743, 0.014999555269428389, 0.20999377377199743, 0.32999021592742456, 0.014999555269428389, 0.10499688688599872, 0.029999110538856778, 0.059998221077713555, 0.014999555269428389, 0.1772907963928588, 0.5318723891785765, 0.1772907963928588, 0.42612038559895987, 0.21306019279947994, 0.21306019279947994, 0.3125512689714678, 0.3125512689714678, 0.1562756344857339, 0.2616300719087982, 0.2616300719087982, 0.1308150359543991, 0.2616300719087982, 0.1308150359543991, 0.21462112650850473, 0.21462112650850473, 0.42924225301700947, 0.2676071835507202, 0.2676071835507202, 0.2676071835507202, 0.21728029833310103, 0.21728029833310103, 0.43456059666620206, 0.3456404220362043, 0.13441571968074614, 0.01920224566867802, 0.24962919369281425, 0.1152134740120681, 0.1152134740120681, 0.214484119749856, 0.214484119749856, 0.428968239499712, 0.2489594158064794, 0.16597294387098627, 0.2489594158064794, 0.16597294387098627, 0.08298647193549313, 0.6207857086820562, 0.15519642717051405, 0.15519642717051405, 0.17896324098319194, 0.17896324098319194, 0.5368897229495758, 0.26900772256186284, 0.5380154451237257, 0.3273349871064792, 0.4364466494753056, 0.2182233247376528, 0.4737073661859318, 0.1579024553953106, 0.3158049107906212, 0.27655464709966177, 0.27655464709966177, 0.27655464709966177, 0.14132577632765678, 0.21198866449148518, 0.21198866449148518, 0.035331444081914194, 0.21198866449148518, 0.035331444081914194, 0.07066288816382839, 0.07066288816382839, 0.359587946264339, 0.1438351785057356, 0.0719175892528678, 0.2876703570114712, 0.0719175892528678, 0.5547204957820562, 0.2773602478910281, 0.5488946238903403, 0.27444731194517014, 0.21528212713785075, 0.4305642542757015, 0.10764106356892537, 0.10764106356892537, 0.10764106356892537, 0.179314203820338, 0.179314203820338, 0.179314203820338, 0.358628407640676, 0.24338916922840817, 0.09735566769136327, 0.09735566769136327, 0.1460335015370449, 0.2920670030740898, 0.09735566769136327, 0.2740071976921476, 0.2740071976921476, 0.2740071976921476, 0.27390805456362477, 0.5478161091272495, 0.30864207638655167, 0.15432103819327583, 0.15432103819327583, 0.15432103819327583, 0.18476894182073753, 0.18476894182073753, 0.18476894182073753, 0.36953788364147505, 0.15597727617483204, 0.31195455234966407, 0.31195455234966407, 0.15597727617483204, 0.5322910448515049, 0.26614552242575246, 0.4342547577318361, 0.21712737886591804, 0.21712737886591804, 0.2196778488765746, 0.043935569775314924, 0.08787113955062985, 0.1757422791012597, 0.08787113955062985, 0.2196778488765746, 0.1757422791012597, 0.09275280450366369, 0.18550560900732738, 0.09275280450366369, 0.27825841351099107, 0.27825841351099107, 0.03861926405812835, 0.11585779217438506, 0.0772385281162567, 0.11585779217438506, 0.3475733765231552, 0.0772385281162567, 0.11585779217438506, 0.11585779217438506, 0.4283569018720599, 0.21417845093602994, 0.21417845093602994, 0.2756762664178921, 0.2756762664178921, 0.2756762664178921, 0.15587074115216057, 0.3507091675923613, 0.05845152793206022, 0.1363868985081405, 0.1363868985081405, 0.09741921322010036, 0.03896768528804014, 0.01948384264402007, 0.01948384264402007, 0.16086145038925836, 0.6434458015570335, 0.16086145038925836, 0.44026618154799846, 0.04891846461644427, 0.1956738584657771, 0.1956738584657771, 0.1467553938493328, 0.2301517105088226, 0.3068689473450968, 0.0767172368362742, 0.3068689473450968, 0.0767172368362742, 0.10400181894032022, 0.2496043654567685, 0.1456025465164483, 0.1456025465164483, 0.04160072757612809, 0.020800363788064043, 0.1456025465164483, 0.10400181894032022, 0.020800363788064043, 0.21681865266794287, 0.43363730533588574, 0.21681865266794287, 0.1463378984180873, 0.1463378984180873, 0.1463378984180873, 0.07316894920904365, 0.1463378984180873, 0.2926757968361746, 0.07316894920904365, 0.5080048146904098, 0.3810036110178074, 0.06350060183630123, 0.3189301526350108, 0.13668435112929034, 0.14807471372339787, 0.034171087782322584, 0.09112290075286023, 0.14807471372339787, 0.04556145037643011, 0.034171087782322584, 0.034171087782322584, 0.5536037857348721, 0.27680189286743606, 0.2731077865681344, 0.2731077865681344, 0.2731077865681344, 0.37705017323432233, 0.5655752598514835, 0.11354272760152372, 0.28385681900380927, 0.05677136380076186, 0.11354272760152372, 0.11354272760152372, 0.11354272760152372, 0.05677136380076186, 0.11354272760152372, 0.27263676708397167, 0.27263676708397167, 0.27263676708397167, 0.2709661493225999, 0.5419322986451998, 0.40145656652844397, 0.26763771101896267, 0.13381885550948133, 0.5864604426771187, 0.053314585697919886, 0.03554305713194659, 0.19548681422570624, 0.10662917139583977, 0.2163309987718607, 0.2163309987718607, 0.2163309987718607, 0.2163309987718607, 0.11332233639834455, 0.11332233639834455, 0.11332233639834455, 0.11332233639834455, 0.4532893455933782, 0.3140160189417343, 0.07850400473543358, 0.23551201420630075, 0.15700800947086715, 0.15700800947086715, 0.07850400473543358, 0.4405161670763181, 0.11012904176907952, 0.11012904176907952, 0.22025808353815904, 0.21765156812084424, 0.21765156812084424, 0.21765156812084424, 0.4353031362416885, 0.2684215950543694, 0.5368431901087388, 0.5359671646402502, 0.2679835823201251, 0.18339821908382006, 0.18339821908382006, 0.3667964381676401, 0.18339821908382006, 0.1812104957349467, 0.3624209914698934, 0.1812104957349467, 0.1812104957349467, 0.11132715183643925, 0.13915893979554905, 0.2504860916319883, 0.13915893979554905, 0.055663575918219625, 0.16699072775465887, 0.13915893979554905, 0.21512325080923372, 0.43024650161846745, 0.21512325080923372, 0.2700406882104167, 0.405061032315625, 0.2700406882104167, 0.4268651279642348, 0.4268651279642348, 0.46097603965733625, 0.3073173597715575, 0.15365867988577875, 0.4341814473865739, 0.21709072369328694, 0.21709072369328694, 0.2734362918604032, 0.5468725837208064, 0.15561941964457487, 0.5187313988152497, 0.05187313988152496, 0.05187313988152496, 0.15561941964457487, 0.4534065369306178, 0.36272522954449427, 0.09068130738612357, 0.09068130738612357, 0.13876195486437576, 0.13876195486437576, 0.4162858645931273, 0.13876195486437576, 0.13876195486437576, 0.15461834104952632, 0.15461834104952632, 0.23192751157428948, 0.30923668209905264, 0.07730917052476316, 0.30483671423219744, 0.15241835711609872, 0.4572550713482961, 0.26940168190286556, 0.5388033638057311, 0.2834321373654159, 0.2834321373654159, 0.2834321373654159, 0.760972936628822, 0.1902432341572055, 0.7121321178275308, 0.05086657984482364, 0.10173315968964727, 0.05086657984482364, 0.18446677786275634, 0.18446677786275634, 0.3689335557255127, 0.18446677786275634, 0.44328449558857164, 0.44328449558857164, 0.3579366307945344, 0.1789683153972672, 0.1789683153972672, 0.1789683153972672, 0.27264558296207064, 0.27264558296207064, 0.27264558296207064, 0.10537243113865155, 0.052686215569325776, 0.07902932335398866, 0.23708797006196597, 0.2107448622773031, 0.1580586467079773, 0.10537243113865155, 0.052686215569325776, 0.13731048219734107, 0.13731048219734107, 0.27462096439468214, 0.41193144659202324, 0.183530719685541, 0.367061439371082, 0.367061439371082, 0.5613209296538233, 0.28066046482691165, 0.4271820145998246, 0.2135910072999123, 0.2135910072999123, 0.21453029190438672, 0.21453029190438672, 0.42906058380877343, 0.1808350902093608, 0.3616701804187216, 0.1808350902093608, 0.1808350902093608, 0.21629917722922765, 0.4325983544584553, 0.21629917722922765, 0.268902762849595, 0.268902762849595, 0.268902762849595, 0.1348893486416546, 0.5395573945666184, 0.1348893486416546, 0.1348893486416546, 0.400111772067191, 0.13337059068906368, 0.400111772067191, 0.2446619996372314, 0.15569399976914725, 0.2001779997031893, 0.13345199980212621, 0.059311999912056095, 0.08155399987907713, 0.022241999967021035, 0.06672599990106311, 0.014827999978014024, 0.022241999967021035, 0.5488891381113705, 0.27444456905568526, 0.2774719309290453, 0.2774719309290453, 0.2774719309290453, 0.2238929090778562, 0.2238929090778562, 0.2238929090778562, 0.2238929090778562, 0.19141575124344704, 0.09570787562172352, 0.04785393781086176, 0.09570787562172352, 0.2392696890543088, 0.3349775646760323, 0.27103490996043555, 0.27103490996043555, 0.27103490996043555, 0.13384789273438377, 0.13384789273438377, 0.13384789273438377, 0.5353915709375351, 0.14087582485030456, 0.2641421715943211, 0.03521895621257614, 0.15848530295659263, 0.14087582485030456, 0.14087582485030456, 0.03521895621257614, 0.01760947810628807, 0.07043791242515228, 0.26905996873445454, 0.5381199374689091, 0.2144726010986745, 0.2144726010986745, 0.428945202197349, 0.18109097557705373, 0.36218195115410745, 0.18109097557705373, 0.18109097557705373, 0.5493701257416211, 0.18312337524720704, 0.18312337524720704, 0.18071214145280728, 0.15812312377120635, 0.11294508840800455, 0.06776705304480272, 0.15812312377120635, 0.09035607072640364, 0.02258901768160091, 0.11294508840800455, 0.06776705304480272, 0.1400184202497305, 0.560073680998922, 0.280036840499461, 0.27718281305902887, 0.5543656261180577, 0.2687316554599621, 0.2687316554599621, 0.2687316554599621, 0.14219515375409933, 0.047398384584699776, 0.047398384584699776, 0.28439030750819866, 0.1895935383387991, 0.09479676916939955, 0.09479676916939955, 0.09479676916939955, 0.2501914469225446, 0.08339714897418152, 0.2501914469225446, 0.16679429794836304, 0.08339714897418152, 0.08339714897418152, 0.2801588624959157, 0.5603177249918314, 0.671403679220138, 0.13428073584402758, 0.13428073584402758, 0.05559699445373556, 0.1389924861343389, 0.1389924861343389, 0.3057834694955456, 0.09729474029403723, 0.09729474029403723, 0.02779849722686778, 0.06949624306716945, 0.06949624306716945, 0.6571926641217214, 0.21906422137390713, 0.5526487053662842, 0.2763243526831421], \"Term\": [\"abl\", \"abl\", \"abl\", \"account\", \"account\", \"account\", \"account\", \"accur\", \"accur\", \"accur\", \"accur\", \"activ\", \"activ\", \"activ\", \"actual\", \"actual\", \"actual\", \"actual\", \"actual\", \"actual\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"allow\", \"allow\", \"allow\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alway\", \"alway\", \"alway\", \"alway\", \"alway\", \"alway\", \"ask\", \"ask\", \"ask\", \"assign\", \"assign\", \"assist\", \"assist\", \"assist\", \"attend\", \"attend\", \"avail\", \"avail\", \"avail\", \"avail\", \"base\", \"base\", \"base\", \"base\", \"base\", \"behind\", \"behind\", \"behind\", \"believ\", \"believ\", \"believ\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"best\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"better\", \"better\", \"better\", \"bia\", \"bia\", \"bia\", \"bia\", \"bia\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias\", \"care\", \"care\", \"case\", \"case\", \"case\", \"case\", \"certain\", \"certain\", \"certain\", \"certain\", \"certain\", \"chanc\", \"chanc\", \"chanc\", \"chanc\", \"chanc\", \"child\", \"child\", \"child\", \"child\", \"children\", \"children\", \"children\", \"children\", \"choic\", \"choic\", \"choic\", \"choos\", \"choos\", \"choos\", \"chosen\", \"chosen\", \"chosen\", \"circumst\", \"circumst\", \"class\", \"class\", \"class\", \"combin\", \"combin\", \"combin\", \"come\", \"come\", \"come\", \"comput\", \"comput\", \"comput\", \"comput\", \"consider\", \"consider\", \"consider\", \"consider\", \"control\", \"control\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"counsel\", \"counsel\", \"counsel\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"creat\", \"creat\", \"cutoff\", \"cutoff\", \"cutoff\", \"cutoff\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"decid\", \"decid\", \"decid\", \"decid\", \"decid\", \"decis\", \"decis\", \"decis\", \"decis\", \"decis\", \"determin\", \"determin\", \"determin\", \"determin\", \"differ\", \"differ\", \"differ\", \"draw\", \"draw\", \"draw\", \"drawn\", \"drawn\", \"drawn\", \"due\", \"due\", \"due\", \"educ\", \"educ\", \"effici\", \"effici\", \"effici\", \"elimin\", \"elimin\", \"elimin\", \"emot\", \"emot\", \"emot\", \"enough\", \"enough\", \"enough\", \"equal\", \"equal\", \"equal\", \"error\", \"error\", \"etc\", \"etc\", \"etc\", \"even\", \"even\", \"even\", \"even\", \"everi\", \"everi\", \"everyon\", \"everyon\", \"everyon\", \"everyon\", \"fact\", \"fact\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"fail\", \"fail\", \"fail\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fall\", \"fall\", \"famili\", \"far\", \"far\", \"favor\", \"favor\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"figur\", \"figur\", \"find\", \"find\", \"futur\", \"futur\", \"gener\", \"gener\", \"gener\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"given\", \"given\", \"good\", \"good\", \"good\", \"good\", \"good\", \"hard\", \"hard\", \"hard\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"home\", \"home\", \"home\", \"household\", \"household\", \"household\", \"howev\", \"howev\", \"howev\", \"human\", \"human\", \"human\", \"human\", \"human\", \"identifi\", \"identifi\", \"identifi\", \"import\", \"import\", \"import\", \"improv\", \"improv\", \"improv\", \"incom\", \"incom\", \"incom\", \"incom\", \"incom\", \"incom\", \"individu\", \"individu\", \"individu\", \"inform\", \"inform\", \"inform\", \"inform\", \"inform\", \"involv\", \"involv\", \"involv\", \"issu\", \"issu\", \"issu\", \"judg\", \"judg\", \"judgement\", \"judgement\", \"judgement\", \"judgment\", \"judgment\", \"judgment\", \"keep\", \"keep\", \"keep\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lead\", \"lead\", \"least\", \"least\", \"less\", \"less\", \"less\", \"less\", \"less\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"limit\", \"limit\", \"limit\", \"line\", \"line\", \"live\", \"live\", \"live\", \"live\", \"look\", \"look\", \"look\", \"look\", \"lot\", \"lot\", \"lot\", \"lot\", \"low\", \"low\", \"made\", \"made\", \"made\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"mani\", \"mani\", \"mani\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mayb\", \"mayb\", \"mayb\", \"measur\", \"measur\", \"measur\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"method\", \"method\", \"method\", \"might\", \"might\", \"might\", \"might\", \"might\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"money\", \"money\", \"money\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"name\", \"name\", \"name\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neither\", \"neither\", \"number\", \"number\", \"number\", \"object\", \"object\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"opinion\", \"opinion\", \"opinion\", \"opportun\", \"opportun\", \"other\", \"other\", \"other\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"past\", \"past\", \"past\", \"past\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"peopl\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"pick\", \"pick\", \"pick\", \"pick\", \"play\", \"play\", \"play\", \"play\", \"plenti\", \"plenti\", \"point\", \"point\", \"poor\", \"poor\", \"poor\", \"poor\", \"possibl\", \"possibl\", \"possibl\", \"possibl\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predictor\", \"predictor\", \"predictor\", \"probabl\", \"probabl\", \"probabl\", \"problem\", \"problem\", \"program\", \"program\", \"program\", \"provid\", \"provid\", \"provid\", \"put\", \"put\", \"random\", \"random\", \"random\", \"random\", \"random\", \"randomli\", \"randomli\", \"randomli\", \"randomli\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"realli\", \"realli\", \"realli\", \"realli\", \"realli\", \"receiv\", \"receiv\", \"receiv\", \"regardless\", \"regardless\", \"relat\", \"relat\", \"relat\", \"remov\", \"remov\", \"request\", \"request\", \"request\", \"request\", \"resourc\", \"resourc\", \"resourc\", \"resourc\", \"result\", \"result\", \"risk\", \"risk\", \"risk\", \"risk\", \"say\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"select\", \"select\", \"select\", \"select\", \"sens\", \"sens\", \"sens\", \"servic\", \"servic\", \"set\", \"set\", \"set\", \"show\", \"show\", \"show\", \"situat\", \"situat\", \"situat\", \"situat\", \"someth\", \"someth\", \"someth\", \"statist\", \"statist\", \"statist\", \"still\", \"still\", \"still\", \"still\", \"struggl\", \"struggl\", \"struggl\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"success\", \"success\", \"sure\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"taken\", \"taken\", \"taken\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"time\", \"time\", \"time\", \"truli\", \"truli\", \"truli\", \"truli\", \"unfair\", \"unfair\", \"unfair\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"variabl\", \"variabl\", \"variabl\", \"varieti\", \"varieti\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"well\", \"well\", \"whole\", \"whole\", \"without\", \"without\", \"without\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wrong\", \"wrong\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [10, 8, 7, 1, 9, 3, 4, 6, 5, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el770591404901720847361594929250\", ldavis_el770591404901720847361594929250_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el770591404901720847361594929250\", ldavis_el770591404901720847361594929250_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el770591404901720847361594929250\", ldavis_el770591404901720847361594929250_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words = 20)\n",
    "    \n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics not capturing more nuanced moral differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see from viz that, for instance, topic 10 is focused on issues of counselor bias, topic 2 on issues with parent\n",
    "## requests--- find high-probability docs for that topic\n",
    "\n",
    "## get document-specific topic probabilities\n",
    "## has element for each document w/ tuples of topic probabilities\n",
    "all_topics = ldamodel.get_document_topics(corpus, minimum_probability=0.0, per_word_topics=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an example text focused on algorithms as efficient, these are the topics\n",
      "----------------------------------------------------------------------------\n",
      "129    I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.\n",
      "Name: explain_fairness, dtype: object\n",
      "[(0, 0.98026466), (1, 0.0018776644), (2, 0.0019383765), (3, 0.0020405487), (4, 0.0019668536), (5, 0.0019466605), (6, 0.0022311981), (7, 0.0024939054), (8, 0.0023879749), (9, 0.0028521467)]\n"
     ]
    }
   ],
   "source": [
    "efficient_nostories = frdata_schools.explain_fairness[frdata_schools.explain_fairness.astype(str).str.contains(\"stories\")]\n",
    "print(\"For an example text focused on algorithms as efficient, these are the topics:\")\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(efficient_nostories)\n",
    "print(all_topics[efficient_nostories.index[0]])\n",
    "focal_text = efficient_nostories.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7      I think that drawing a random name is fair but it may not be the name of a student that will actually benefit from having a mentor. I think an algorithm is also fair and will most likely end up choosing students that will for sure benefit from a mentor.                                                                                                                                                         \n",
       "60     Drawing students' names randomly would perhaps be the correct way to determine if the counselor is indeed helpful to students and helps them stay in school. However, I think the predictive model/algorithm is probably more ethical because it would provide more opportunities for students who would probably be more susceptible to missing school and ensure more of these kids would have access to a counselor\n",
       "124    Be an excellent communicator. Being able to communicate ideas, thoughts, and feelings verbally is a trait that can never go unsung as a school counselor. with this i think the school counselor using a predictive model/algorithm is more fair than the school counselor drawing students' names randomly.                                                                                                          \n",
       "129    I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.                                                        \n",
       "130    Because drawing  the students name at random would be unfair to the students who are failing. The mentor programs is for helping them. Selecting students at random would waste the school limited resources.                                                                                                                                                                                                         \n",
       "Name: explain_fairness, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## see topic 0 is the top topic--- look at other responses with high-probability of that topic\n",
    "## empty topic dictionary\n",
    "topic_dict = {i: [] for i in range(len(all_topics[0]))}  \n",
    "\n",
    "## iterate over docs and append the vector of \n",
    "## document-specific probabilities to each key (one topic)\n",
    "for docID in range(len(all_topics)):\n",
    "    topic_vector = ldamodel[corpus[docID]]\n",
    "    for topicID, prob in topic_vector:\n",
    "        topic_dict[topicID].append([docID, prob])\n",
    "     \n",
    "## pull out that topic and pull topic responses\n",
    "focal_topic = 0\n",
    "top_0 = topic_dict[0]\n",
    "indices_top_5_resp = [el[0] for el in \n",
    "                                sorted(top_0, key=itemgetter(1))[len(top_0)-5:len(parent_issues)]]\n",
    "\n",
    "\n",
    "## see that they're either talking about benefits of a predictive model, \n",
    "## esp relative to randomness, but not the focal responses ideas about\n",
    "## efficiency (regardless of its normative desirability)\n",
    "frdata_schools.explain_fairness[frdata_schools.explain_fairness.index.isin(indices_top_5_resp)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: moral dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create input file that matches formatting\n",
    "frdata_schools.explain_fairness.to_csv(\"fr_forscore.csv\",\n",
    "                                      index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eMFDscore\n",
      "Total number of input texts to be scored: 212\n",
      "Scoring completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processed: 0 N/A% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\r",
      "Processed: 202  95% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤ | Elapsed Time: 0:00:00 ETA:   0:00:00\r",
      "Processed: 212 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "emfdscore fr_forscore.csv all-vv.csv bow emfd all vice-virtue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an example text focused on algorithms as efficient, these are the moral foundation scores:\n",
      "----------------------------------------------------------------------------\n",
      "129    I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.\n",
      "Name: explain_fairness, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>care.virtue</th>\n",
       "      <th>fairness.virtue</th>\n",
       "      <th>loyalty.virtue</th>\n",
       "      <th>authority.virtue</th>\n",
       "      <th>sanctity.virtue</th>\n",
       "      <th>care.vice</th>\n",
       "      <th>fairness.vice</th>\n",
       "      <th>loyalty.vice</th>\n",
       "      <th>authority.vice</th>\n",
       "      <th>sanctity.vice</th>\n",
       "      <th>moral_nonmoral_ratio</th>\n",
       "      <th>f_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.041613</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>0.02459</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>0.056424</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     care.virtue  fairness.virtue  loyalty.virtue  authority.virtue  \\\n",
       "129  0.04         0.041613         0.061589        0.025956           \n",
       "\n",
       "     sanctity.virtue  care.vice  fairness.vice  loyalty.vice  authority.vice  \\\n",
       "129  0.011745         0.039836   0.02459        0.022727      0.033713         \n",
       "\n",
       "     sanctity.vice  moral_nonmoral_ratio     f_var  \n",
       "129  0.056424       4.0                   0.000237  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfdt_res = pd.read_csv(\"all-vv.csv\")\n",
    "\n",
    "## pull up results for focal text\n",
    "## see that it's generally low in all\n",
    "## but is highest in loyalty,\n",
    "## somewhat high in virtue\n",
    "focal_mfdt = mfdt_res.iloc[focal_text]\n",
    "print(\"For an example text focused on algorithms as efficient, these are the moral foundation scores:\")\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(efficient_nostories)\n",
    "focal_mfdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explain_fairness</th>\n",
       "      <th>loyalty.virtue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.</td>\n",
       "      <td>0.061589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>I think it would be fair.</td>\n",
       "      <td>0.061347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>I think it is more fair because students call fall into certain algorithms and models.</td>\n",
       "      <td>0.061264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>It helps gauge the general state of the class as a whole</td>\n",
       "      <td>0.061202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>If initially set up correctly, all students will me measured using the same algorithm. There wouldn't be any chance that one student is more favored than the other other than based on their needs.</td>\n",
       "      <td>0.061062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                  explain_fairness  \\\n",
       "75  I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.   \n",
       "76  I think it would be fair.                                                                                                                                                                                                                                                                                                                                        \n",
       "77  I think it is more fair because students call fall into certain algorithms and models.                                                                                                                                                                                                                                                                           \n",
       "78  It helps gauge the general state of the class as a whole                                                                                                                                                                                                                                                                                                         \n",
       "79  If initially set up correctly, all students will me measured using the same algorithm. There wouldn't be any chance that one student is more favored than the other other than based on their needs.                                                                                                                                                             \n",
       "\n",
       "    loyalty.virtue  \n",
       "75  0.061589        \n",
       "76  0.061347        \n",
       "77  0.061264        \n",
       "78  0.061202        \n",
       "79  0.061062        "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add index as row number to each\n",
    "answer_only = pd.DataFrame(frdata_schools.explain_fairness.copy())\n",
    "answer_only['doc'] = answer_only.index\n",
    "mfdt_res['doc'] = mfdt_res.index\n",
    "\n",
    "mfdt_wanswer = pd.merge(answer_only,\n",
    "                       mfdt_res,\n",
    "                       on = \"doc\",\n",
    "                       how = \"left\")\n",
    "\n",
    "## shows other answers that score similarly high in virtue \n",
    "## but that touch on very different moral concerns\n",
    "prox_answer = mfdt_wanswer.sort_values(by = 'loyalty.virtue', ascending = False).reset_index()\n",
    "\n",
    "prox_answer.loc[prox_answer.index.isin(range(prox_answer[prox_answer.doc == focal_text[0]].index[0],\n",
    "                                        prox_answer[prox_answer.doc == focal_text[0]].index[0]+5)),\n",
    "            ['explain_fairness', 'loyalty.virtue']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
