{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages and imports\n",
    "\n",
    "## outputs and loading\n",
    "import warnings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## dataframe\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "## preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "## add punctuation and some application-specific words\n",
    "## to stopword list\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "## lda estimate and visualize\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "## functions\n",
    "stopwords_standard = set(stopwords.words('english'))\n",
    "def remove_stop(row, colname, stopword_dict):\n",
    "    \n",
    "    string_of_col = str(row[colname])\n",
    "    try:\n",
    "        processed_string = \" \".join([i for i in wordpunct_tokenize(string_of_col) if \n",
    "                        i not in stopword_dict])  ## removed numeric\n",
    "        return(processed_string)\n",
    "    except:\n",
    "        processed_string = \"\" # to handle data errors where not actually text\n",
    "        return(processed_string)\n",
    "    \n",
    "def processtext(row, colname):\n",
    "    \n",
    "    string_of_col = str(row[colname])\n",
    "    try:\n",
    "        processed_string = \" \".join([porter.stem(i.lower()) for i in wordpunct_tokenize(string_of_col) if \n",
    "                        i.lower().isalpha() and len(i) >=3])  \n",
    "        return(processed_string)\n",
    "    except:\n",
    "        processed_string = \"\" # to handle data errors where not actually text eg someone left blank\n",
    "        return(processed_string)\n",
    "    \n",
    "def create_dtm(list_of_strings, metadata):\n",
    "    vectorizer = CountVectorizer(lowercase = True)\n",
    "    dtm_sparse = vectorizer.fit_transform(list_of_strings)\n",
    "    dtm_dense_named = pd.DataFrame(dtm_sparse.todense(), columns=vectorizer.get_feature_names())\n",
    "    dtm_dense_named_withid = pd.concat([metadata.reset_index(), dtm_dense_named], axis = 1)\n",
    "    return(dtm_dense_named_withid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyze free responses from optimizing schools pilot\n",
    "\n",
    "*Note of caution*: these are based on low N and are meant to be illustrative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdata_schools = pd.read_csv(\"../data/cleaned_fr_pilot.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdata_schools['text_lower'] = frdata_schools.explain_fairness.astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to lowercase and remove stopwords\n",
    "frdata_schools['text_lower_nostop'] = frdata_schools.apply(remove_stop,\n",
    "                                    axis = 1,\n",
    "                                   args = [\"text_lower\", stopwords_standard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove punctuation, digits, and length2 or less\n",
    "frdata_schools['text_preprocess'] = frdata_schools.apply(processtext,\n",
    "                                    axis = 1,\n",
    "                                   args = [\"text_lower_nostop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "## store in a list and re-tokenize\n",
    "all_stemmed_text = frdata_schools.text_preprocess\n",
    "text_preprocess_tokens = [wordpunct_tokenize(one_row) for one_row \n",
    "                         in all_stemmed_text]\n",
    "\n",
    "## create dictionary (all unique words and counts)\n",
    "dictionary = corpora.Dictionary(text_preprocess_tokens)\n",
    "\n",
    "## filter out words that are in almost none or almost all documents\n",
    "lower_thres = 0.01*frdata_schools.shape[0]\n",
    "upper_thres = 0.99*frdata_schools.shape[0]\n",
    "dictionary.filter_extremes(no_below=lower_thres, no_above=upper_thres)\n",
    "\n",
    "## use the dictionary to create the corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in text_preprocess_tokens]\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "## estimate\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = num_topics, id2word=dictionary, passes=10,\n",
    "                                          alpha = 'auto',\n",
    "                                          per_word_topics = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el770591404897043557608363110774\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el770591404897043557608363110774_data = {\"mdsDat\": {\"x\": [0.06464887121557508, -0.05337183274893277, -0.032340438562610097, 0.09189901183607847, -0.04776789486231591, 0.055240759110503294, -0.10273881878629072, -0.11216816587308664, 0.17445216778684963, -0.03785365911577028], \"y\": [-0.05822640707747122, 0.003503974155199751, -0.039060324889272496, 0.019549251480475312, -0.035549759895750474, 0.009403053126114265, 0.031035830075488532, 0.16696117209402572, 0.07957902830906065, -0.17719581737786974], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [21.57813323427959, 14.538180958115104, 13.12525835482162, 10.29253776130268, 9.80102597543746, 9.637248737397929, 7.529559918385519, 5.237812510782069, 5.0705840430415074, 3.1896585064365226]}, \"tinfo\": {\"Term\": [\"help\", \"incom\", \"parent\", \"need\", \"mentor\", \"name\", \"fair\", \"student\", \"actual\", \"base\", \"school\", \"bia\", \"counselor\", \"accur\", \"factor\", \"human\", \"request\", \"take\", \"account\", \"certain\", \"much\", \"draw\", \"miss\", \"model\", \"bias\", \"mani\", \"decis\", \"might\", \"predict\", \"random\", \"equal\", \"everyon\", \"keep\", \"chanc\", \"draw\", \"someth\", \"thought\", \"name\", \"opportun\", \"kid\", \"fair\", \"opinion\", \"plenti\", \"lead\", \"issu\", \"home\", \"poor\", \"everi\", \"still\", \"resourc\", \"fall\", \"give\", \"person\", \"howev\", \"method\", \"gener\", \"mayb\", \"individu\", \"wrong\", \"predict\", \"would\", \"randomli\", \"think\", \"algorithm\", \"random\", \"student\", \"good\", \"get\", \"model\", \"may\", \"mentor\", \"better\", \"use\", \"need\", \"incom\", \"help\", \"like\", \"might\", \"way\", \"parent\", \"limit\", \"differ\", \"problem\", \"peopl\", \"import\", \"fail\", \"judg\", \"say\", \"control\", \"regardless\", \"sens\", \"unfair\", \"probabl\", \"well\", \"past\", \"benefit\", \"ask\", \"made\", \"choic\", \"other\", \"counselor\", \"miss\", \"school\", \"assist\", \"life\", \"home\", \"poor\", \"counsel\", \"bias\", \"comput\", \"incom\", \"use\", \"make\", \"student\", \"could\", \"help\", \"know\", \"think\", \"parent\", \"would\", \"need\", \"data\", \"may\", \"random\", \"realli\", \"algorithm\", \"like\", \"much\", \"child\", \"want\", \"assign\", \"circumst\", \"counsel\", \"mani\", \"neither\", \"actual\", \"fact\", \"result\", \"time\", \"individu\", \"request\", \"thing\", \"due\", \"remov\", \"risk\", \"best\", \"pick\", \"decid\", \"come\", \"situat\", \"child\", \"class\", \"creat\", \"figur\", \"though\", \"activ\", \"varieti\", \"say\", \"regardless\", \"method\", \"one\", \"would\", \"variabl\", \"randomli\", \"need\", \"much\", \"like\", \"model\", \"help\", \"parent\", \"fair\", \"get\", \"student\", \"better\", \"draw\", \"kid\", \"think\", \"determin\", \"mentor\", \"counselor\", \"realli\", \"use\", \"way\", \"school\", \"incom\", \"may\", \"relat\", \"alway\", \"error\", \"whole\", \"behind\", \"put\", \"mani\", \"find\", \"improv\", \"avail\", \"effici\", \"human\", \"rather\", \"even\", \"accur\", \"choos\", \"truli\", \"good\", \"inform\", \"better\", \"howev\", \"allow\", \"comput\", \"attend\", \"least\", \"line\", \"taken\", \"creat\", \"figur\", \"though\", \"way\", \"decis\", \"factor\", \"make\", \"think\", \"may\", \"school\", \"use\", \"algorithm\", \"incom\", \"fair\", \"model\", \"data\", \"predict\", \"would\", \"mentor\", \"student\", \"counselor\", \"take\", \"predictor\", \"elimin\", \"bias\", \"hard\", \"futur\", \"activ\", \"varieti\", \"without\", \"believ\", \"consider\", \"less\", \"probabl\", \"favor\", \"lot\", \"take\", \"number\", \"point\", \"success\", \"statist\", \"though\", \"fail\", \"import\", \"factor\", \"decis\", \"inform\", \"judgment\", \"educ\", \"effici\", \"made\", \"combin\", \"child\", \"actual\", \"know\", \"parent\", \"model\", \"make\", \"might\", \"get\", \"need\", \"help\", \"student\", \"predict\", \"would\", \"algorithm\", \"counselor\", \"data\", \"one\", \"use\", \"account\", \"least\", \"number\", \"allow\", \"drawn\", \"object\", \"receiv\", \"emot\", \"decis\", \"take\", \"decid\", \"famili\", \"provid\", \"without\", \"success\", \"measur\", \"creat\", \"assign\", \"mentor\", \"bia\", \"determin\", \"factor\", \"children\", \"abl\", \"opportun\", \"set\", \"wrong\", \"one\", \"get\", \"base\", \"algorithm\", \"student\", \"need\", \"help\", \"incom\", \"could\", \"use\", \"think\", \"fair\", \"predict\", \"model\", \"parent\", \"favor\", \"improv\", \"set\", \"money\", \"identifi\", \"show\", \"enough\", \"human\", \"care\", \"ask\", \"know\", \"sure\", \"point\", \"success\", \"measur\", \"figur\", \"year\", \"struggl\", \"request\", \"given\", \"judgment\", \"might\", \"miss\", \"fact\", \"mayb\", \"truli\", \"choos\", \"judgement\", \"give\", \"parent\", \"mentor\", \"children\", \"data\", \"school\", \"need\", \"use\", \"student\", \"think\", \"counselor\", \"bia\", \"base\", \"way\", \"algorithm\", \"help\", \"kid\", \"chanc\", \"educ\", \"involv\", \"low\", \"due\", \"select\", \"given\", \"play\", \"mean\", \"remov\", \"feel\", \"etc\", \"come\", \"far\", \"variabl\", \"certain\", \"bia\", \"case\", \"error\", \"miss\", \"care\", \"enough\", \"provid\", \"children\", \"everi\", \"famili\", \"receiv\", \"program\", \"live\", \"could\", \"parent\", \"request\", \"school\", \"also\", \"mentor\", \"student\", \"child\", \"model\", \"take\", \"may\", \"need\", \"would\", \"accur\", \"attend\", \"system\", \"etc\", \"certain\", \"base\", \"famili\", \"less\", \"behind\", \"point\", \"far\", \"taken\", \"cutoff\", \"situat\", \"incom\", \"avail\", \"believ\", \"account\", \"emot\", \"program\", \"live\", \"random\", \"person\", \"much\", \"children\", \"rather\", \"thing\", \"bia\", \"data\", \"factor\", \"predict\", \"fair\", \"model\", \"might\", \"like\", \"student\", \"algorithm\", \"way\", \"draw\", \"mentor\", \"would\", \"sure\", \"truli\", \"choos\", \"possibl\", \"whole\", \"line\", \"statist\", \"believ\", \"class\", \"servic\", \"name\", \"given\", \"find\", \"gener\", \"fact\", \"actual\", \"much\", \"struggl\", \"other\", \"help\", \"feel\", \"randomli\", \"give\", \"determin\", \"miss\", \"counselor\", \"need\", \"draw\", \"like\", \"might\", \"student\", \"may\", \"fair\", \"algorithm\", \"would\", \"predict\", \"make\"], \"Freq\": [66.0, 51.0, 55.0, 86.0, 50.0, 15.0, 51.0, 133.0, 18.0, 22.0, 37.0, 21.0, 35.0, 9.0, 24.0, 15.0, 19.0, 20.0, 11.0, 10.0, 13.0, 17.0, 12.0, 48.0, 11.0, 11.0, 15.0, 20.0, 36.0, 19.0, 7.711429094638051, 6.776815214848256, 2.9460859232593357, 10.54106520504225, 11.301599017967364, 2.9644963339666255, 2.908307839321362, 9.617206451250595, 2.1894585860730746, 16.341959723723555, 28.743748067769904, 2.0091506818503073, 2.0083379471980916, 2.0093623119473913, 2.963015157910206, 2.9621932729544582, 2.9449724017063867, 2.905150749979499, 3.9224172099694097, 2.87063744741727, 1.8866244808917039, 5.0846908762697325, 5.836817202610699, 2.889061466268274, 2.915769325294156, 2.0093630158168936, 2.0081364058972104, 2.009302013793344, 1.9451532243211835, 15.018851172233354, 28.1862158503207, 4.5649325195414905, 20.509961460954056, 18.132840238226635, 7.6109845704001735, 29.403694236324966, 4.876460468882943, 8.43284090212176, 12.796702305609516, 7.797636685072106, 11.759471760297139, 6.325942220097637, 9.803212882965571, 14.246120708568986, 9.570565242537189, 9.29157952652781, 5.042984731261495, 4.456090362892499, 4.388463050339002, 4.384766327711892, 2.9032859200137042, 3.652730340319612, 2.9084113898769783, 5.722634525271683, 1.9702371598063635, 1.968482511537108, 1.9537320845730743, 1.9701491112400595, 1.9702328917430059, 1.9680732516840358, 2.9083930530121824, 2.7984856792601103, 3.514567438681382, 5.671106038278371, 2.1770958057973138, 3.647880239581117, 1.9695414654790668, 1.9702371598063635, 1.970236053271419, 2.90721475137263, 13.631405246837863, 4.744690638910919, 13.885420144103268, 1.970238266341308, 1.9699369726835423, 1.9696756723602018, 1.9537300295796058, 2.1860441958176757, 3.838403110185298, 1.9694935683236088, 12.825536712791, 10.161202904071454, 5.967604922052968, 23.98616197880311, 5.910168803066987, 12.5235715463902, 3.8448418789513625, 9.092689132497178, 8.83065596687054, 10.31437073571097, 11.333750561911993, 4.554223095053967, 4.77428159639876, 3.6838008931047703, 2.9895159751745894, 4.451558466743404, 3.2474129131784055, 2.996417907853169, 2.9108467152135686, 2.8395205692826853, 2.0082570064279017, 2.0092531462607113, 3.380635143540399, 5.623401733932436, 1.867564528399628, 8.644178365888001, 2.0025811500825323, 2.008257719995117, 2.006539878280571, 2.0079387554497976, 7.145662095814485, 2.762048862124932, 2.0035256276489415, 2.0039644714864613, 2.0072742816587428, 1.9972363889251457, 3.2659357784704075, 2.3552257825457517, 2.892410171294046, 1.7206043613774662, 5.067873101922031, 1.05125455146916, 1.0511461606091288, 1.051912389085111, 1.0492003341692369, 1.0519780372689351, 1.051912246371668, 1.052003012121477, 1.0519116041611738, 1.83773542080437, 4.742730777077833, 18.581009431037376, 2.008257719995117, 2.9562929898270887, 19.351739659868578, 3.466288041537071, 4.87242932501721, 10.085063522516737, 13.154379850738097, 10.996253463750524, 8.266027701447786, 5.355647909679757, 13.492566184281266, 3.9199549535377383, 3.7421382848434708, 4.6742049193871065, 6.044261394207319, 2.940770619475314, 4.481951644216139, 3.826838998747806, 2.7783179092109846, 3.7957351746788155, 2.9725440550193114, 3.0358388941667984, 3.0261095478971747, 2.9610148068053617, 2.5975580432893164, 5.6777576432597145, 2.7824770436700526, 1.8848912839910141, 1.8848877027836355, 1.8848885980854801, 4.765424023944296, 1.8848899410382471, 1.8848906125146305, 1.8848908363400918, 1.8848906125146305, 6.01445873214614, 2.770264007381403, 2.7840062192207546, 3.7059726149084744, 1.8865710940770823, 1.8848901648637082, 4.570890600879074, 3.6199876019205917, 5.8946959949878845, 1.9564704426471815, 1.8853624365867734, 1.8855336630645672, 0.987297354682643, 0.9872957879044149, 0.9872955640789537, 0.9872970189444513, 0.987295452166223, 0.9872908637442691, 0.9892069216046006, 5.667832327009666, 4.034908303245126, 6.201842722330722, 4.577647891551746, 9.046169231859956, 4.585491631012928, 6.014983379027118, 6.458852098720934, 6.0178720704289566, 5.593503024912124, 5.1184017003668805, 4.96910206005101, 3.3902887368257546, 3.9348256435855564, 4.1459919843211175, 3.4548339565392463, 4.303813107594589, 2.81498948250844, 2.0104761690451793, 2.8472004395809924, 2.769408896615453, 6.420611012171241, 1.9287849800217425, 1.9284526996601834, 1.928496179579207, 1.9174644289480742, 3.4620832238459074, 2.8464352782609175, 3.594095717792089, 3.68739637736392, 2.8427588805979718, 1.9286745751293193, 2.232865203904176, 6.819275405784139, 1.010291512372479, 1.0098805845103327, 1.0102907663934761, 1.010291192667192, 1.0102904466881892, 1.0101539325306659, 1.0100088928988245, 6.444529230371471, 3.7615356071314636, 2.8452886019650947, 1.4939724687304614, 1.0098242098114025, 1.0101781235640444, 1.010291192667192, 1.0102904466881892, 3.5919114912717185, 3.764951338417125, 2.846897145832116, 10.081203724551873, 8.579301605589452, 4.365822931913489, 3.8063088406093284, 4.614195575231013, 9.793925931729829, 7.7769930888142875, 11.043968488378507, 4.6067421793081635, 6.39300126359115, 4.8728260692484175, 3.766005087042877, 3.0731370184085374, 2.8340795214681394, 3.3221812559654915, 7.161073839177161, 1.859034096187125, 1.867847785240489, 3.231635758020893, 1.859034934488307, 2.6373202487893987, 2.763483738395124, 2.638823113233619, 4.92742117775771, 6.395324271177982, 2.2021059209097444, 1.8780798798939755, 1.3507445303250785, 2.153742649111703, 0.9837322908685888, 0.9828803672922739, 0.9837157344202422, 0.9837327100191798, 13.299720348008563, 5.207780948498775, 2.7707465702615925, 5.433865328354998, 2.2016775490056886, 2.609746427156987, 0.8122000984687746, 0.9837312429921112, 0.9837316621427022, 3.378404271532186, 5.156036808033301, 4.239665475139281, 9.410992896734191, 23.82718203999831, 13.213687174291419, 8.609746303479929, 6.546058787581817, 3.608596536756022, 4.269258345168795, 4.737402610508629, 4.0081862080256165, 3.382959600155748, 3.4204354353511923, 2.5912285636195675, 2.767970190612255, 1.8805804757347337, 1.8885426942023564, 1.8885422029802905, 1.8885412205361585, 1.838465715647818, 1.8312724234541573, 5.893233043291567, 1.7368757526991654, 1.721423216428647, 4.451042717904975, 0.9892178386578343, 0.9892173474357684, 0.9892174293061127, 0.9892176749171456, 0.989216774343358, 0.98921775678749, 1.8889756245831817, 4.84288663016822, 0.9892181661392117, 1.4294652177671368, 4.40964677944129, 2.761682875648967, 0.9892176749171456, 0.9892178386578343, 0.989216528732325, 0.9892178386578343, 1.7054098682994572, 1.880579329549913, 10.20958234322086, 8.86027130362742, 1.8138450107376727, 3.695139723356355, 5.511910452719061, 9.538787828862818, 5.315610583078003, 11.67085337105737, 4.929769404461149, 3.6888717297943296, 2.7187986980660237, 2.7878633743593175, 2.4213719241438527, 3.3018264021530466, 3.4814656567226288, 2.461547012032865, 1.8885422029802905, 2.6149856901702675, 3.61443718456641, 1.8423615726305147, 2.733249388494287, 3.127117120266268, 1.8468949308703881, 1.85765630322684, 2.6033725476400025, 1.8502332138677304, 2.7271842563446187, 1.598417237410928, 2.687089550299771, 0.9732606436343761, 1.8580566739357132, 2.742869903661528, 5.4194648435394654, 1.8580663557309491, 0.9725757420490404, 2.7194912157536373, 0.9709383796194379, 0.9709689057502994, 0.9732606436343761, 1.8450709945514159, 0.9732606436343761, 0.9732607575378495, 0.9718038182100549, 0.9732607575378495, 0.9732607575378495, 3.6172483222890253, 7.89266143980413, 2.736699524702472, 4.428271350962188, 1.4321484646239016, 4.508479443221516, 7.513517326613028, 1.8434755486000112, 3.022026586443703, 1.9954644707383826, 1.8574852202098477, 1.910433477609891, 1.7486457813684642, 5.233504194754344, 1.8626004389169513, 1.7564578910805788, 1.8705354626365358, 3.5588097310825453, 7.416415745401894, 1.870536785838846, 2.639342909882747, 0.9797892989104269, 0.9797923863824837, 0.979791945315047, 0.9797910631801736, 2.7404838632540316, 1.2467770115493473, 11.464395948342053, 0.979792606916202, 1.048913056786279, 1.8705359037039726, 0.9797883065086944, 0.9797924966493429, 0.9797918350481878, 2.945473041922941, 1.8705336983667893, 1.870536124237691, 1.3595175974378693, 0.9797901810453002, 0.9797921658487653, 2.639341586680437, 2.7612746795505174, 2.6744849579001957, 3.3898374576917387, 3.792106397303834, 3.0529236855096458, 1.8705354626365358, 1.8705341394342259, 4.89746329502944, 1.8705350215690992, 1.4364531006847543, 1.1980928700235245, 1.3273988461651411, 1.2497675590368942, 1.7384643401236906, 1.7584888985639875, 1.7399252748629248, 1.7509827931669932, 0.9211031220203955, 0.9211056191075958, 0.9211033994745289, 1.5860680793796753, 0.9211033994745289, 0.9211043012004623, 3.387425583727514, 0.9211052029263957, 0.9211053416534624, 0.9211031913839288, 0.9211033301109955, 3.12659622457946, 2.148478983330324, 0.9211054110169957, 0.9211054110169957, 8.159986547270453, 0.9211033301109955, 1.0310380796483398, 0.9211056884711291, 0.9211042318369289, 0.9224109020780121, 2.525960742129226, 5.285854162263954, 1.058222966271925, 1.1229195823629463, 0.9301946696989717, 3.4905502909249435, 1.050697855266795, 1.1727976506450681, 0.9815469211551958, 0.9329281478208853, 0.9321830447457332, 0.9252104836471999], \"Total\": [66.0, 51.0, 55.0, 86.0, 50.0, 15.0, 51.0, 133.0, 18.0, 22.0, 37.0, 21.0, 35.0, 9.0, 24.0, 15.0, 19.0, 20.0, 11.0, 10.0, 13.0, 17.0, 12.0, 48.0, 11.0, 11.0, 15.0, 20.0, 36.0, 19.0, 8.559432047558513, 7.604650578651852, 3.7771011944973485, 15.971489637088563, 17.904649841438932, 4.733884977214382, 4.671884506381291, 15.616561355487324, 3.727151901813677, 29.26109088521878, 51.53420147495126, 3.7591639066220943, 3.759211227518059, 3.7773628820557668, 5.6317283638807485, 5.6542951497075995, 5.652398573733219, 5.596652345623391, 7.567481992500524, 5.629871782969519, 3.7515032187022936, 10.173717883294344, 12.995859893352245, 6.486739186314881, 6.573550161110832, 4.555989892437265, 4.676479957897789, 4.733684977795781, 4.611635892920449, 36.143471472893445, 73.38899956317515, 11.160167235560134, 56.79075976804813, 50.78175482955816, 19.62904243896051, 133.62977031900553, 12.950592590735111, 27.010898657231298, 48.24787079245629, 25.9420714885803, 50.366432090683986, 19.514254635071108, 44.41166454870204, 86.72029721810013, 51.49127018039645, 66.1083113846448, 20.433291089805387, 20.219225538389836, 21.175776935507407, 55.30335941550874, 3.7214631140013665, 5.553699528455389, 4.640494176031417, 9.368572290817228, 3.702259417154967, 3.702121727417278, 3.680674165069275, 3.740105662646531, 3.7406014364700053, 3.74000872447969, 5.575874686462462, 5.637189207394287, 7.421886314680363, 12.082372995974868, 4.690368283172978, 8.384429801679465, 4.586323948535394, 4.599806093599437, 4.6379162511120136, 7.429876960044008, 35.04996832665827, 12.689633929813976, 37.73789484872537, 5.494090909997809, 5.615117792155823, 5.6542951497075995, 5.652398573733219, 6.596967660781602, 11.987059394123513, 6.39660722120575, 51.49127018039645, 44.41166454870204, 22.965338467716354, 133.62977031900553, 23.91714345949755, 66.1083113846448, 13.767731383184984, 56.79075976804813, 55.30335941550874, 73.38899956317515, 86.72029721810013, 22.85750883593771, 25.9420714885803, 19.62904243896051, 12.998378850351012, 50.78175482955816, 20.433291089805387, 13.64141503004561, 16.65122924464045, 3.768600048477381, 3.7143325883001608, 3.7767347970109655, 6.596967660781602, 11.120940142251134, 3.717620697634207, 18.448726709366305, 4.5563994926179845, 4.695248253036868, 4.714736517285081, 4.733684977795781, 19.34596543810782, 7.51627323355014, 5.4749503817577985, 5.489357360716909, 5.592776401530315, 5.5740315800163645, 9.37076284163453, 7.380743919739605, 9.222395859784987, 5.507031781979356, 16.65122924464045, 3.6391980603406213, 3.6555160292645894, 3.660506993215261, 3.6795311728406084, 3.700624501301979, 3.700842446021612, 3.740105662646531, 3.74000872447969, 6.573550161110832, 17.569636778705657, 73.38899956317515, 7.299892494885904, 11.160167235560134, 86.72029721810013, 13.64141503004561, 20.433291089805387, 48.24787079245629, 66.1083113846448, 55.30335941550874, 51.53420147495126, 27.010898657231298, 133.62977031900553, 19.514254635071108, 17.904649841438932, 29.26109088521878, 56.79075976804813, 12.03323742039992, 50.366432090683986, 35.04996832665827, 12.998378850351012, 44.41166454870204, 21.175776935507407, 37.73789484872537, 51.49127018039645, 25.9420714885803, 3.612203491994897, 9.131270956890141, 4.484950262844453, 3.5399087070139794, 3.593255735524725, 3.659136620846545, 11.120940142251134, 4.496692615194033, 4.501319788619951, 4.531432478909151, 4.57719907856426, 15.326112270306032, 7.261809626803073, 7.318142969694612, 9.853519790563734, 5.279023557004592, 5.276577853339441, 12.950592590735111, 11.820431834006826, 19.514254635071108, 6.486739186314881, 6.278474621477839, 6.39660722120575, 3.5869671212850887, 3.594909173045752, 3.599098365357542, 3.633834172175438, 3.6555160292645894, 3.660506993215261, 3.6795311728406084, 21.175776935507407, 15.42482520556361, 24.47563162422264, 22.965338467716354, 56.79075976804813, 25.9420714885803, 37.73789484872537, 44.41166454870204, 50.78175482955816, 51.49127018039645, 51.53420147495126, 48.24787079245629, 22.85750883593771, 36.143471472893445, 73.38899956317515, 50.366432090683986, 133.62977031900553, 35.04996832665827, 20.87983999724613, 4.560328479832352, 4.622640649508633, 11.987059394123513, 3.6825181813666896, 3.682504179661997, 3.700624501301979, 3.700842446021612, 7.300226296114106, 6.247568411899548, 8.30798186294583, 9.043333246734049, 7.421886314680363, 5.443489322527523, 6.494665748088825, 20.87983999724613, 3.614460171505186, 3.6158460642920702, 3.619436369911161, 3.61999915999247, 3.6795311728406084, 3.702121727417278, 3.702259417154967, 24.47563162422264, 15.42482520556361, 11.820431834006826, 6.503562801623867, 4.482945382815211, 4.57719907856426, 4.599806093599437, 4.617719521331114, 16.65122924464045, 18.448726709366305, 13.767731383184984, 55.30335941550874, 48.24787079245629, 22.965338467716354, 20.219225538389836, 27.010898657231298, 86.72029721810013, 66.1083113846448, 133.62977031900553, 36.143471472893445, 73.38899956317515, 50.78175482955816, 35.04996832665827, 22.85750883593771, 17.569636778705657, 44.41166454870204, 11.639876708527567, 3.594909173045752, 3.614460171505186, 6.278474621477839, 3.654146432721851, 5.428297238219769, 6.349526775861735, 6.319022418561649, 15.42482520556361, 20.87983999724613, 7.380743919739605, 6.300466633685169, 4.573469830972568, 7.300226296114106, 3.619436369911161, 3.6391541473480293, 3.6555160292645894, 3.7143325883001608, 50.366432090683986, 21.704681388897676, 12.03323742039992, 24.47563162422264, 9.95760901621421, 11.959913119218486, 3.727151901813677, 4.538487916483322, 4.611635892920449, 17.569636778705657, 27.010898657231298, 22.748228018982065, 50.78175482955816, 133.62977031900553, 86.72029721810013, 66.1083113846448, 51.49127018039645, 23.91714345949755, 44.41166454870204, 56.79075976804813, 51.53420147495126, 36.143471472893445, 48.24787079245629, 55.30335941550874, 5.443489322527523, 4.501319788619951, 4.538487916483322, 4.560338947131597, 4.619040928267301, 4.54480256930017, 4.549465827590428, 15.326112270306032, 4.532716580299549, 4.586323948535394, 13.767731383184984, 3.4841948495352857, 3.6158460642920702, 3.619436369911161, 3.6391541473480293, 3.660506993215261, 3.701637332712668, 7.2729887416797006, 19.34596543810782, 4.414545734402808, 6.503562801623867, 20.219225538389836, 12.689633929813976, 4.5563994926179845, 4.676479957897789, 5.276577853339441, 5.279023557004592, 9.17238166005046, 10.173717883294344, 55.30335941550874, 50.366432090683986, 9.95760901621421, 22.85750883593771, 37.73789484872537, 86.72029721810013, 44.41166454870204, 133.62977031900553, 56.79075976804813, 35.04996832665827, 21.704681388897676, 22.748228018982065, 21.175776935507407, 50.78175482955816, 66.1083113846448, 29.26109088521878, 15.971489637088563, 4.482945382815211, 6.322278445933875, 3.616248010372969, 5.4749503817577985, 7.264278403588989, 4.414545734402808, 4.531337247460564, 7.32147925148923, 5.489357360716909, 9.10921628540601, 5.436144284514398, 9.222395859784987, 3.621077094693216, 7.299892494885904, 10.829957273668537, 21.704681388897676, 8.339107033367881, 4.484950262844453, 12.689633929813976, 4.532716580299549, 4.549465827590428, 4.573469830972568, 9.95760901621421, 5.596652345623391, 6.300466633685169, 6.349526775861735, 6.393296960191497, 6.43406048558569, 23.91714345949755, 55.30335941550874, 19.34596543810782, 37.73789484872537, 10.151284511718051, 50.366432090683986, 133.62977031900553, 16.65122924464045, 48.24787079245629, 20.87983999724613, 25.9420714885803, 86.72029721810013, 73.38899956317515, 9.853519790563734, 3.5869671212850887, 4.542993898786426, 5.436144284514398, 10.829957273668537, 22.748228018982065, 6.300466633685169, 9.043333246734049, 3.593255735524725, 3.6158460642920702, 3.621077094693216, 3.633834172175438, 12.0051772213957, 5.507031781979356, 51.49127018039645, 4.531432478909151, 6.247568411899548, 11.639876708527567, 6.319022418561649, 6.393296960191497, 6.43406048558569, 19.62904243896051, 12.995859893352245, 13.64141503004561, 9.95760901621421, 7.261809626803073, 7.51627323355014, 21.704681388897676, 22.85750883593771, 24.47563162422264, 36.143471472893445, 51.53420147495126, 48.24787079245629, 20.219225538389836, 20.433291089805387, 133.62977031900553, 50.78175482955816, 21.175776935507407, 17.904649841438932, 50.366432090683986, 73.38899956317515, 3.4841948495352857, 5.276577853339441, 5.279023557004592, 5.352717206193381, 3.5399087070139794, 3.599098365357542, 3.61999915999247, 6.247568411899548, 3.6391980603406213, 3.6397239336257687, 15.616561355487324, 4.414545734402808, 4.496692615194033, 4.555989892437265, 4.5563994926179845, 18.448726709366305, 13.64141503004561, 7.2729887416797006, 7.429876960044008, 66.1083113846448, 9.10921628540601, 11.160167235560134, 10.173717883294344, 12.03323742039992, 12.689633929813976, 35.04996832665827, 86.72029721810013, 17.904649841438932, 20.433291089805387, 20.219225538389836, 133.62977031900553, 25.9420714885803, 51.53420147495126, 50.78175482955816, 73.38899956317515, 36.143471472893445, 22.965338467716354], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.179599761962891, -4.308800220489502, -5.1417999267578125, -3.867000102996826, -3.797300100326538, -5.1356000900268555, -5.154699802398682, -3.958699941635132, -5.438600063323975, -3.428499937057495, -2.8638999462127686, -5.524600028991699, -5.525000095367432, -5.524499893188477, -5.136099815368652, -5.136300086975098, -5.142199993133545, -5.155799865722656, -4.855599880218506, -5.167699813842773, -5.587500095367432, -4.5960001945495605, -4.458099842071533, -5.161300182342529, -5.152100086212158, -5.524499893188477, -5.525100231170654, -5.524499893188477, -5.5569000244140625, -3.513000011444092, -2.8833999633789062, -4.70389986038208, -3.2014000415802, -3.3245999813079834, -4.192699909210205, -2.841200113296509, -4.637899875640869, -4.090099811553955, -3.673099994659424, -4.168499946594238, -3.7576000690460205, -4.377600193023682, -3.9395999908447266, -3.5657999515533447, -3.963599920272827, -3.9932000637054443, -4.604300022125244, -4.728000164031982, -4.743299961090088, -4.744100093841553, -4.761499881744385, -4.531899929046631, -4.759799957275391, -4.082900047302246, -5.149199962615967, -5.150100231170654, -5.157599925994873, -5.1493000984191895, -5.149199962615967, -5.150300025939941, -4.759799957275391, -4.798299789428711, -4.570499897003174, -4.0920000076293945, -5.0493998527526855, -4.533199787139893, -5.149600028991699, -5.149199962615967, -5.149199962615967, -4.760200023651123, -3.2149999141693115, -4.270299911499023, -3.196500062942505, -5.149199962615967, -5.149400234222412, -5.149499893188477, -5.157599925994873, -5.045300006866455, -4.4822998046875, -5.149600028991699, -3.275899887084961, -3.5088000297546387, -4.040999889373779, -2.649899959564209, -4.0507001876831055, -3.299799919128418, -4.480599880218506, -3.6198999881744385, -3.6491000652313232, -3.493799924850464, -3.399600028991699, -4.311299800872803, -4.264100074768066, -4.523399829864502, -4.7322998046875, -4.334099769592285, -4.649499893188477, -4.730000019073486, -4.758900165557861, -4.68149995803833, -5.027900218963623, -5.027400016784668, -4.5071001052856445, -3.998199939727783, -5.100500106811523, -3.56820011138916, -5.030700206756592, -5.027900218963623, -5.02869987487793, -5.0279998779296875, -3.7585999965667725, -4.709199905395508, -5.030200004577637, -5.03000020980835, -5.02839994430542, -5.033400058746338, -4.541600227355957, -4.868500232696533, -4.663000106811523, -5.182499885559082, -4.102200031280518, -5.67519998550415, -5.675300121307373, -5.674499988555908, -5.67710018157959, -5.674499988555908, -5.674499988555908, -5.6743998527526855, -5.674499988555908, -5.116600036621094, -4.168499946594238, -2.802999973297119, -5.027900218963623, -4.641200065612793, -2.762399911880493, -4.482100009918213, -4.141499996185303, -3.414099931716919, -3.148400068283081, -3.3276000022888184, -3.61299991607666, -4.046999931335449, -3.122999906539917, -4.359099864959717, -4.4054999351501465, -4.18310022354126, -3.9260001182556152, -4.646500110626221, -4.225100040435791, -4.3831000328063965, -4.7032999992370605, -4.391300201416016, -4.635700225830078, -4.61460018157959, -4.6178998947143555, -4.639599800109863, -4.527400016784668, -3.745500087738037, -4.458700180053711, -4.848100185394287, -4.848100185394287, -4.848100185394287, -3.920599937438965, -4.848100185394287, -4.848100185394287, -4.848100185394287, -4.848100185394287, -3.6877999305725098, -4.463099956512451, -4.458099842071533, -4.172100067138672, -4.847300052642822, -4.848100185394287, -3.9623000621795654, -4.195499897003174, -3.7079999446868896, -4.8109002113342285, -4.847899913787842, -4.847799777984619, -5.494800090789795, -5.494800090789795, -5.494800090789795, -5.494800090789795, -5.494800090789795, -5.494800090789795, -5.4928998947143555, -3.7472000122070312, -4.086999893188477, -3.6572000980377197, -3.9607999324798584, -3.2797000408172607, -3.9591000080108643, -3.6877999305725098, -3.6166000366210938, -3.687299966812134, -3.7604000568389893, -3.8492000102996826, -3.8787999153137207, -4.261099815368652, -4.112100124359131, -4.059899806976318, -4.242199897766113, -4.022500038146973, -4.4471001625061035, -4.783599853515625, -4.38670015335083, -4.414400100708008, -3.5736000537872314, -4.776199817657471, -4.776400089263916, -4.776299953460693, -4.782100200653076, -4.191199779510498, -4.38700008392334, -4.153800010681152, -4.128200054168701, -4.388299942016602, -4.776299953460693, -4.629799842834473, -3.5132999420166016, -5.422800064086914, -5.423299789428711, -5.422800064086914, -5.422800064086914, -5.422800064086914, -5.422999858856201, -5.423099994659424, -3.5699000358581543, -4.10830020904541, -4.387400150299072, -5.031599998474121, -5.423299789428711, -5.422999858856201, -5.422800064086914, -5.422800064086914, -4.154399871826172, -4.1072998046875, -4.386899948120117, -3.1224000453948975, -3.2836999893188477, -3.9593000411987305, -4.096399784088135, -3.903899908065796, -3.1512999534606934, -3.3819000720977783, -3.0311999320983887, -3.905600070953369, -3.577899932861328, -3.849400043487549, -4.107100009918213, -4.310400009155273, -4.39139986038208, -4.232500076293945, -3.4475998878479004, -4.796199798583984, -4.791399955749512, -4.243199825286865, -4.796199798583984, -4.446499824523926, -4.399700164794922, -4.445899963378906, -3.8213999271392822, -3.560699939727783, -4.626800060272217, -4.785999774932861, -5.115600109100342, -4.64900016784668, -5.432600021362305, -5.433499813079834, -5.432700157165527, -5.432600021362305, -2.8285000324249268, -3.7660999298095703, -4.39709997177124, -3.723599910736084, -4.626999855041504, -4.456999778747559, -5.624199867248535, -5.432600021362305, -5.432600021362305, -4.198800086975098, -3.776099920272827, -3.9716999530792236, -3.1744000911712646, -2.2453999519348145, -2.8350000381469727, -3.2632999420166016, -3.537400007247925, -4.132900238037109, -3.9647998809814453, -3.8606998920440674, -4.027900218963623, -4.197500228881836, -4.186500072479248, -4.464099884033203, -4.151299953460693, -4.537899971008301, -4.533599853515625, -4.533599853515625, -4.533599853515625, -4.560500144958496, -4.5644001960754395, -3.3956000804901123, -4.617300033569336, -4.626299858093262, -3.676300048828125, -5.180300235748291, -5.180300235748291, -5.180300235748291, -5.180300235748291, -5.180300235748291, -5.180300235748291, -4.533400058746338, -3.591900110244751, -5.180300235748291, -4.812099933624268, -3.6856000423431396, -4.153600215911865, -5.180300235748291, -5.180300235748291, -5.180300235748291, -5.180300235748291, -4.6356000900268555, -4.537899971008301, -2.846100091934204, -2.9879000186920166, -4.573999881744385, -3.8624000549316406, -3.4625000953674316, -2.914099931716919, -3.498800039291382, -2.7123000621795654, -3.5741000175476074, -3.8640999794006348, -4.1691999435424805, -4.144199848175049, -4.285099983215332, -3.9749999046325684, -3.921999931335449, -4.268599987030029, -4.533599853515625, -3.8452000617980957, -3.5216000080108643, -4.195499897003174, -3.8010001182556152, -3.6663999557495117, -4.192999839782715, -4.18720006942749, -3.8496999740600586, -4.191199779510498, -3.8032000064849854, -4.337500095367432, -3.818000078201294, -4.833600044250488, -4.186999797821045, -3.797499895095825, -3.116499900817871, -4.186999797821045, -4.8343000411987305, -3.8060998916625977, -4.835999965667725, -4.835999965667725, -4.833600044250488, -4.193999767303467, -4.833600044250488, -4.833600044250488, -4.835100173950195, -4.833600044250488, -4.833600044250488, -3.5208001136779785, -2.740600109100342, -3.7997000217437744, -3.31850004196167, -4.447299957275391, -3.30049991607666, -2.789799928665161, -4.194799900054932, -3.7005999088287354, -4.115600109100342, -4.187300205230713, -4.159200191497803, -4.247700214385986, -3.11899995803833, -4.152100086212158, -4.2108001708984375, -4.147799968719482, -3.5046000480651855, -2.770400047302246, -4.147799968719482, -3.803499937057495, -4.79449987411499, -4.79449987411499, -4.79449987411499, -4.79449987411499, -3.765899896621704, -4.553500175476074, -2.3348000049591064, -4.79449987411499, -4.72629976272583, -4.147799968719482, -4.79449987411499, -4.79449987411499, -4.79449987411499, -3.6937999725341797, -4.147799968719482, -4.147799968719482, -4.466899871826172, -4.79449987411499, -4.79449987411499, -3.803499937057495, -3.7583999633789062, -3.790299892425537, -3.553299903869629, -3.4410998821258545, -3.6579999923706055, -4.147799968719482, -4.147799968719482, -3.18530011177063, -4.147799968719482, -4.411900043487549, -4.593299865722656, -4.490799903869629, -4.55109977722168, -3.757499933242798, -3.7460999488830566, -3.756700038909912, -3.7502999305725098, -4.3927001953125, -4.3927001953125, -4.3927001953125, -3.849299907684326, -4.3927001953125, -4.3927001953125, -3.090399980545044, -4.3927001953125, -4.3927001953125, -4.3927001953125, -4.3927001953125, -3.170599937438965, -3.545799970626831, -4.3927001953125, -4.3927001953125, -2.2112998962402344, -4.3927001953125, -4.279900074005127, -4.3927001953125, -4.3927001953125, -4.391300201416016, -3.3838999271392822, -2.6454999446868896, -4.253900051116943, -4.1946001052856445, -4.382900238037109, -3.060499906539917, -4.261099815368652, -4.151100158691406, -4.329100131988525, -4.379899978637695, -4.38070011138916, -4.388199806213379], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4292, 1.4182, 1.285, 1.118, 1.0734, 1.0655, 1.0595, 1.0487, 1.0015, 0.951, 0.9497, 0.907, 0.9066, 0.9023, 0.8913, 0.887, 0.8815, 0.8778, 0.8763, 0.8599, 0.8461, 0.8399, 0.733, 0.7247, 0.7206, 0.7149, 0.6882, 0.6766, 0.6702, 0.6553, 0.5765, 0.6395, 0.515, 0.5037, 0.5861, 0.0195, 0.5568, 0.3694, 0.2063, 0.3314, 0.0788, 0.407, 0.0227, -0.2727, -0.1492, -0.4287, 0.1343, 0.0211, -0.0404, -1.0012, 1.6801, 1.5094, 1.4612, 1.4355, 1.2976, 1.2967, 1.295, 1.2874, 1.2873, 1.2864, 1.2775, 1.2281, 1.1809, 1.172, 1.1609, 1.0962, 1.0831, 1.0805, 1.0723, 0.9901, 0.984, 0.9446, 0.9286, 0.9029, 0.8809, 0.8738, 0.8661, 0.8239, 0.7896, 0.7504, 0.5384, 0.4535, 0.5808, 0.2108, 0.5305, 0.2647, 0.6528, 0.0965, 0.0938, -0.0338, -0.1065, 0.3152, 0.2358, 0.2553, 0.4587, -0.5059, 0.0891, 0.4127, 0.1844, 1.7476, 1.4157, 1.3995, 1.3621, 1.3487, 1.3422, 1.2725, 1.2085, 1.1813, 1.1764, 1.173, 1.0347, 1.0295, 1.0254, 1.0229, 1.0059, 1.0043, 0.9766, 0.8884, 0.8711, 0.8673, 0.8411, 0.7889, 0.7843, 0.7836, 0.7759, 0.7728, 0.7727, 0.7622, 0.7622, 0.7561, 0.7211, 0.657, 0.74, 0.7022, 0.5307, 0.6606, 0.5971, 0.4653, 0.4161, 0.4154, 0.2005, 0.4125, -0.2623, 0.4256, 0.4652, 0.1964, -0.2096, 0.6216, -0.3886, -0.1841, 0.4877, -0.429, 0.0672, -0.4895, -0.8035, -0.1397, 1.944, 1.7986, 1.7964, 1.6435, 1.6286, 1.6104, 1.4263, 1.4043, 1.4033, 1.3966, 1.3865, 1.3384, 1.3101, 1.3073, 1.2959, 1.2448, 1.2443, 1.2323, 1.0904, 1.0767, 1.0751, 1.0707, 1.0522, 0.9837, 0.9814, 0.9803, 0.9707, 0.9647, 0.9634, 0.9601, 0.9557, 0.9328, 0.9009, 0.661, 0.4367, 0.5408, 0.4373, 0.3457, 0.1409, 0.0539, -0.0357, 0.0006, 0.3654, 0.0561, -0.5999, -0.4058, -1.1618, -0.2481, -0.0667, 1.8516, 1.8104, 1.6984, 1.676, 1.6758, 1.6709, 1.6651, 1.5766, 1.5366, 1.4848, 1.4256, 1.363, 1.2851, 1.255, 1.2037, 1.048, 1.0472, 1.0466, 1.0464, 1.0301, 1.0239, 1.0237, 0.9882, 0.9115, 0.8985, 0.8518, 0.8322, 0.8117, 0.8069, 0.803, 0.7889, 0.7334, 0.7466, 0.6205, 0.5957, 0.6625, 0.6527, 0.5556, 0.1418, 0.1826, -0.1705, 0.2627, -0.1179, -0.0212, 0.0919, 0.3161, 0.4982, -0.2702, 1.8538, 1.6801, 1.6794, 1.6754, 1.6637, 1.6177, 1.5076, 1.4663, 1.1984, 1.1563, 1.1301, 1.1292, 1.1199, 1.1188, 1.0368, 1.0305, 1.0269, 1.0109, 1.008, 0.9122, 0.871, 0.8345, 0.8304, 0.8172, 0.8159, 0.8105, 0.7945, 0.6908, 0.6835, 0.6595, 0.6539, 0.6153, 0.4581, 0.3011, 0.277, 0.4483, -0.0025, -0.1444, -0.2144, -0.0292, -0.307, -0.7212, 1.91, 1.7135, 1.7095, 1.7047, 1.692, 1.6813, 1.6763, 1.6306, 1.6271, 1.6064, 1.4571, 1.3273, 1.2902, 1.2892, 1.2837, 1.2779, 1.2667, 1.2382, 1.2014, 1.0906, 1.0713, 1.0635, 1.0614, 1.059, 1.0329, 0.9122, 0.9118, 0.9039, 0.8981, 0.8968, 0.8486, 0.8834, 0.7641, 0.6626, 0.379, 0.4635, 0.1484, 0.1423, 0.3349, 0.509, 0.4871, 0.4178, -0.1467, -0.3575, 0.1109, 0.4513, 2.4102, 2.3901, 2.2749, 2.2546, 2.1064, 2.0779, 2.0576, 1.9153, 1.8618, 1.7432, 1.7252, 1.7161, 1.6354, 1.5809, 1.576, 1.5617, 1.4478, 1.4207, 1.4089, 1.4085, 1.4048, 1.4019, 1.2634, 1.2, 1.0815, 1.0723, 1.0669, 1.0606, 1.0604, 1.0024, 0.9935, 0.8066, 0.9908, 0.5359, 0.0709, 0.7484, 0.1788, 0.6014, 0.3126, -0.8661, -0.7877, 2.349, 2.3264, 2.0314, 1.9149, 1.8688, 1.8609, 1.7673, 1.7502, 1.6822, 1.676, 1.6745, 1.671, 1.5045, 1.4963, 1.4795, 1.4503, 1.1973, 1.1535, 1.1177, 1.106, 1.0997, 1.085, 1.0433, 0.9948, 0.9905, 0.9787, 0.9442, 0.8747, 0.8681, 0.7678, 0.615, 0.3724, 0.2215, 0.6013, 0.5908, -0.3246, -0.3196, 0.291, 0.2774, -0.6544, -1.0911, 2.75, 2.3464, 2.3354, 2.3278, 2.099, 2.0824, 2.0766, 2.0743, 2.0713, 2.0712, 1.917, 1.8782, 1.8597, 1.8466, 1.8465, 1.6702, 1.5969, 1.3789, 1.3576, 1.3532, 1.1538, 1.0635, 1.0433, 0.8754, 0.8237, 0.8151, 0.6476, 0.6168, 0.544, 0.3663, -0.1998, 0.2388, -0.3376, -0.5009, -0.9199, -0.2125, 0.2335]}, \"token.table\": {\"Topic\": [1, 2, 3, 5, 6, 7, 8, 4, 6, 9, 4, 9, 3, 5, 2, 3, 5, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 4, 6, 9, 1, 2, 3, 4, 5, 7, 8, 1, 2, 4, 2, 7, 3, 6, 2, 4, 5, 6, 4, 9, 2, 4, 9, 1, 2, 3, 5, 6, 7, 9, 4, 9, 5, 9, 10, 1, 2, 8, 9, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 6, 7, 8, 9, 2, 5, 7, 2, 7, 8, 1, 2, 3, 4, 8, 1, 4, 6, 8, 9, 1, 3, 4, 7, 9, 2, 3, 4, 5, 7, 8, 1, 2, 6, 7, 8, 9, 1, 2, 4, 4, 7, 10, 1, 3, 2, 3, 10, 2, 3, 4, 5, 1, 2, 3, 6, 8, 2, 4, 5, 7, 1, 4, 5, 1, 2, 1, 2, 3, 4, 5, 6, 8, 9, 2, 3, 1, 2, 3, 4, 5, 7, 8, 10, 3, 4, 6, 1, 2, 3, 4, 6, 9, 1, 2, 4, 5, 6, 7, 8, 9, 1, 3, 4, 6, 2, 4, 5, 6, 1, 3, 5, 6, 10, 2, 4, 1, 3, 9, 10, 1, 6, 3, 8, 5, 8, 3, 4, 5, 1, 5, 2, 6, 7, 9, 3, 7, 8, 1, 4, 8, 1, 8, 9, 1, 2, 4, 8, 1, 2, 8, 1, 3, 7, 10, 1, 4, 5, 6, 7, 9, 2, 5, 1, 3, 4, 6, 9, 10, 1, 2, 2, 6, 8, 9, 2, 8, 9, 5, 7, 1, 2, 5, 8, 10, 3, 4, 7, 1, 4, 10, 2, 5, 1, 4, 10, 1, 2, 3, 5, 6, 7, 1, 2, 6, 7, 10, 7, 8, 10, 1, 2, 4, 6, 2, 5, 1, 2, 3, 4, 5, 6, 7, 9, 10, 1, 2, 1, 4, 5, 1, 4, 5, 7, 8, 1, 3, 7, 2, 5, 4, 7, 1, 2, 3, 4, 5, 6, 8, 9, 1, 3, 2, 4, 5, 6, 7, 3, 5, 8, 1, 3, 4, 2, 4, 1, 2, 4, 5, 6, 7, 1, 3, 5, 7, 1, 1, 2, 3, 4, 5, 7, 2, 3, 5, 6, 7, 1, 3, 4, 6, 4, 5, 6, 9, 1, 2, 3, 5, 1, 2, 3, 5, 6, 8, 9, 10, 2, 1, 4, 10, 2, 3, 5, 8, 9, 1, 3, 4, 5, 2, 8, 2, 4, 5, 1, 2, 4, 5, 6, 7, 10, 3, 4, 1, 2, 3, 4, 5, 6, 8, 10, 1, 3, 7, 1, 2, 3, 4, 8, 2, 6, 7, 1, 2, 3, 4, 6, 7, 8, 9, 1, 3, 4, 1, 2, 3, 5, 6, 7, 9, 10, 2, 4, 7, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 4, 7, 2, 3, 4, 6, 9, 10, 1, 6, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 4, 5, 6, 2, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 1, 6, 1, 2, 10, 1, 2, 3, 5, 6, 7, 8, 1, 2, 1, 2, 5, 1, 3, 4, 5, 6, 7, 9, 1, 2, 3, 4, 9, 1, 4, 8, 1, 2, 5, 7, 9, 1, 2, 1, 5, 6, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 4, 5, 2, 5, 2, 5, 2, 3, 4, 5, 8, 9, 1, 6, 8, 1, 4, 1, 2, 3, 4, 9, 1, 3, 6, 10, 2, 4, 6, 9, 1, 2, 3, 4, 5, 6, 3, 5, 6, 8, 2, 3, 4, 3, 7, 8, 2, 3, 4, 5, 7, 8, 1, 3, 4, 1, 3, 5, 1, 3, 4, 5, 2, 3, 1, 2, 3, 4, 7, 8, 9, 1, 2, 7, 8, 2, 3, 4, 1, 2, 10, 2, 6, 7, 2, 4, 7, 3, 4, 5, 9, 1, 3, 1, 5, 10, 1, 2, 3, 1, 4, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 7, 7, 10, 3, 4, 9, 3, 4, 5, 6, 8, 9, 2, 4, 9, 1, 3, 5, 9, 1, 2, 3, 4, 5, 6, 7, 9, 3, 4, 5, 1, 4, 1, 2, 3, 4, 7, 10, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 3, 4, 5, 6, 8, 3, 5, 3, 1, 2, 3, 4, 5, 6, 7, 9, 2, 3, 4, 5, 4, 10, 2, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 6, 1, 2, 7], \"Freq\": [0.08361264751941146, 0.25083794255823433, 0.1672252950388229, 0.08361264751941146, 0.25083794255823433, 0.1672252950388229, 0.08361264751941146, 0.17182312580121806, 0.6013809403042631, 0.17182312580121806, 0.4059463100516241, 0.5074328875645302, 0.27022466063448836, 0.5404493212689767, 0.10840856561578378, 0.48783854527102705, 0.21681713123156757, 0.16261284842367568, 0.3544580147026127, 0.07876844771169171, 0.039384223855845855, 0.11815267156753757, 0.09846055963961464, 0.17722900735130634, 0.05907633578376879, 0.039384223855845855, 0.019692111927922928, 0.3185487113634675, 0.4778230670452012, 0.15927435568173376, 0.19701940160295148, 0.09850970080147574, 0.19701940160295148, 0.09850970080147574, 0.09850970080147574, 0.09850970080147574, 0.09850970080147574, 0.21902756028621287, 0.10951378014310643, 0.6570826808586386, 0.43607909568592157, 0.43607909568592157, 0.5384547432020046, 0.2692273716010023, 0.36402746746700587, 0.18201373373350294, 0.18201373373350294, 0.18201373373350294, 0.27878705496518014, 0.5575741099303603, 0.2206807680914026, 0.4413615361828052, 0.2206807680914026, 0.1318784037814583, 0.08791893585430553, 0.043959467927152766, 0.08791893585430553, 0.17583787170861107, 0.1318784037814583, 0.30771627549006936, 0.5565982905772608, 0.2782991452886304, 0.48018681864867524, 0.16006227288289174, 0.3201245457657835, 0.35780608472612857, 0.4770747796348381, 0.11926869490870952, 0.11926869490870952, 0.1794033610403521, 0.3588067220807042, 0.1794033610403521, 0.1794033610403521, 0.3074675467858646, 0.05124459113097742, 0.2049783645239097, 0.3074675467858646, 0.10248918226195484, 0.046073009876639676, 0.09214601975327935, 0.09214601975327935, 0.23036504938319838, 0.13821902962991903, 0.23036504938319838, 0.13821902962991903, 0.33369318266337644, 0.5005397739950647, 0.08342329566584411, 0.22061825006802302, 0.44123650013604604, 0.22061825006802302, 0.3597507488506719, 0.11991691628355729, 0.11991691628355729, 0.11991691628355729, 0.23983383256711457, 0.18467293540138968, 0.09233646770069484, 0.09233646770069484, 0.2770094031020845, 0.36934587080277936, 0.6887272414750905, 0.06261156740682641, 0.06261156740682641, 0.12522313481365283, 0.06261156740682641, 0.1801668787285247, 0.3002781312142078, 0.06005562624284157, 0.24022250497136627, 0.06005562624284157, 0.12011125248568313, 0.10042571448343436, 0.10042571448343436, 0.20085142896686872, 0.20085142896686872, 0.20085142896686872, 0.10042571448343436, 0.2156140701678764, 0.4312281403357528, 0.2156140701678764, 0.378857941890836, 0.189428970945418, 0.378857941890836, 0.26477898336717565, 0.5295579667343513, 0.27478581363785465, 0.27478581363785465, 0.27478581363785465, 0.2165571112278681, 0.2165571112278681, 0.2165571112278681, 0.2165571112278681, 0.10843169336946183, 0.10843169336946183, 0.3252950801083855, 0.10843169336946183, 0.3252950801083855, 0.3126657508952071, 0.3126657508952071, 0.15633287544760355, 0.15633287544760355, 0.2407323502859506, 0.2407323502859506, 0.4814647005719012, 0.26733668822618456, 0.5346733764523691, 0.16724405265092773, 0.2508660789763916, 0.08362202632546387, 0.08362202632546387, 0.08362202632546387, 0.16724405265092773, 0.16724405265092773, 0.04181101316273193, 0.3031695928858081, 0.45475438932871215, 0.11412278501141139, 0.3994297475399399, 0.11412278501141139, 0.08559208875855855, 0.11412278501141139, 0.11412278501141139, 0.028530696252852848, 0.08559208875855855, 0.27355918890640957, 0.27355918890640957, 0.27355918890640957, 0.24989218773492006, 0.16659479182328002, 0.08329739591164001, 0.08329739591164001, 0.08329739591164001, 0.24989218773492006, 0.131247898514787, 0.218746497524645, 0.131247898514787, 0.131247898514787, 0.043749299504929, 0.174997198019716, 0.043749299504929, 0.131247898514787, 0.13548769756467588, 0.27097539512935176, 0.13548769756467588, 0.27097539512935176, 0.12966111274172598, 0.25932222548345196, 0.25932222548345196, 0.32415278185431495, 0.33241262182850384, 0.24930946637137788, 0.08310315545712596, 0.24930946637137788, 0.08310315545712596, 0.7202406215001862, 0.18006015537504655, 0.6143655473530316, 0.22340565358292058, 0.055851413395730146, 0.055851413395730146, 0.27366172057180904, 0.5473234411436181, 0.36530011425562475, 0.547950171383437, 0.22306762956188808, 0.6692028886856642, 0.21847422033338174, 0.43694844066676347, 0.21847422033338174, 0.21632657085432233, 0.648979712562967, 0.15825232666726674, 0.4747569800018002, 0.15825232666726674, 0.15825232666726674, 0.2198060251239734, 0.4396120502479468, 0.2198060251239734, 0.9346414523241545, 0.6689037389898131, 0.22296791299660437, 0.18395391065109087, 0.36790782130218175, 0.36790782130218175, 0.13664668812035116, 0.2732933762407023, 0.40994006436105346, 0.13664668812035116, 0.5360347248200997, 0.1786782416066999, 0.1786782416066999, 0.9204893673419714, 0.43894307407423, 0.219471537037115, 0.219471537037115, 0.08171392798789605, 0.24514178396368816, 0.24514178396368816, 0.20428481996974013, 0.040856963993948024, 0.12257089198184408, 0.5402307507039391, 0.27011537535196956, 0.5627330815263676, 0.15523671214520485, 0.09702294509075303, 0.07761835607260242, 0.07761835607260242, 0.019404589018150606, 0.5331196278946104, 0.2665598139473052, 0.15871840264236045, 0.3174368052847209, 0.15871840264236045, 0.3174368052847209, 0.276160925009171, 0.276160925009171, 0.276160925009171, 0.3674113939606956, 0.5511170909410434, 0.21955785627839633, 0.21955785627839633, 0.10977892813919816, 0.3293367844175945, 0.10977892813919816, 0.27318620121570514, 0.27318620121570514, 0.27318620121570514, 0.22238566999689166, 0.4447713399937833, 0.22238566999689166, 0.2715543421574028, 0.5431086843148056, 0.4389825366645147, 0.21949126833225735, 0.21949126833225735, 0.2961767433775573, 0.03702209292219466, 0.18511046461097327, 0.18511046461097327, 0.18511046461097327, 0.07404418584438932, 0.49146241888721937, 0.09829248377744387, 0.09829248377744387, 0.19658496755488775, 0.09829248377744387, 0.22652387361330129, 0.45304774722660257, 0.22652387361330129, 0.38608271899287555, 0.15443308759715021, 0.38608271899287555, 0.07721654379857511, 0.2715533096509712, 0.5431066193019424, 0.13614021915692828, 0.1966469832266742, 0.1966469832266742, 0.030253382034872954, 0.12101352813949182, 0.13614021915692828, 0.045380073052309435, 0.015126691017436477, 0.12101352813949182, 0.5305701100790855, 0.3537134067193903, 0.46248198267769436, 0.30832132178512955, 0.15416066089256478, 0.06524811918137097, 0.3914887150882258, 0.06524811918137097, 0.3914887150882258, 0.06524811918137097, 0.21649515895827337, 0.21649515895827337, 0.43299031791654674, 0.5402106591268845, 0.27010532956344224, 0.44431413316963536, 0.44431413316963536, 0.19420767763089986, 0.2524699809201698, 0.05826230328926996, 0.11652460657853991, 0.019420767763089986, 0.1359453743416299, 0.019420767763089986, 0.21362844539398984, 0.42250382300076317, 0.42250382300076317, 0.16919855620215957, 0.33839711240431913, 0.2537978343032394, 0.16919855620215957, 0.08459927810107978, 0.15817082536805105, 0.15817082536805105, 0.6326833014722042, 0.5326961469307693, 0.17756538231025643, 0.17756538231025643, 0.543378715502886, 0.271689357751443, 0.21804587664628397, 0.10902293832314199, 0.21804587664628397, 0.10902293832314199, 0.10902293832314199, 0.21804587664628397, 0.30752374675318306, 0.15376187337659153, 0.15376187337659153, 0.15376187337659153, 0.7942598954908954, 0.5468012133506064, 0.0683501516688258, 0.17087537917206452, 0.0341750758344129, 0.0683501516688258, 0.0683501516688258, 0.2905344307403717, 0.07263360768509293, 0.2179008230552788, 0.07263360768509293, 0.2905344307403717, 0.5294699139182343, 0.26473495695911714, 0.27817114476713184, 0.5563422895342637, 0.11057869623030255, 0.4423147849212102, 0.11057869623030255, 0.33173608869090765, 0.17809065401922192, 0.35618130803844383, 0.17809065401922192, 0.17809065401922192, 0.24469871143247252, 0.1468192268594835, 0.24469871143247252, 0.097879484572989, 0.0489397422864945, 0.0489397422864945, 0.097879484572989, 0.0489397422864945, 0.8061345519489403, 0.27784736578063984, 0.27784736578063984, 0.27784736578063984, 0.31084569448494087, 0.15542284724247044, 0.15542284724247044, 0.15542284724247044, 0.15542284724247044, 0.3079450240512434, 0.1539725120256217, 0.1539725120256217, 0.3079450240512434, 0.27652970624015993, 0.5530594124803199, 0.43480093710536427, 0.21740046855268214, 0.21740046855268214, 0.17417553003292424, 0.26126329504938633, 0.2177194125411553, 0.17417553003292424, 0.08708776501646212, 0.04354388250823106, 0.04354388250823106, 0.5395227312846108, 0.449602276070509, 0.308379383023503, 0.19273711438968935, 0.1156422686338136, 0.19273711438968935, 0.07709484575587575, 0.03854742287793787, 0.07709484575587575, 0.03854742287793787, 0.42767209910144827, 0.21383604955072413, 0.21383604955072413, 0.13658442039518098, 0.13658442039518098, 0.13658442039518098, 0.13658442039518098, 0.40975326118554295, 0.27478912942688416, 0.27478912942688416, 0.27478912942688416, 0.23825392234244794, 0.03970898705707466, 0.07941797411414932, 0.059563480585611984, 0.25810841587098526, 0.17869044175683596, 0.09927246764268664, 0.01985449352853733, 0.45637439838034866, 0.30424959892023246, 0.15212479946011623, 0.1978315139917343, 0.04945787849793357, 0.09891575699586715, 0.1978315139917343, 0.04945787849793357, 0.1978315139917343, 0.09891575699586715, 0.04945787849793357, 0.394022398727565, 0.078804479745513, 0.236413439236539, 0.236413439236539, 0.078804479745513, 0.2694419419236339, 0.020726303224894917, 0.20726303224894915, 0.10363151612447458, 0.18653672902405424, 0.06217890967468474, 0.020726303224894917, 0.06217890967468474, 0.06217890967468474, 0.21928194627484632, 0.21928194627484632, 0.43856389254969264, 0.219918534359699, 0.219918534359699, 0.14661235623979935, 0.07330617811989967, 0.14661235623979935, 0.14661235623979935, 0.6403458336547446, 0.12806916673094892, 0.19210375009642341, 0.16143856108784116, 0.1268445837118752, 0.21909519004778444, 0.02306265158397731, 0.11531325791988654, 0.1499072352958525, 0.11531325791988654, 0.02306265158397731, 0.05765662895994327, 0.5379784982563568, 0.2689892491281784, 0.27666648753901346, 0.5533329750780269, 0.1842198310289939, 0.5526594930869817, 0.1842198310289939, 0.11383274595773053, 0.056916372978865266, 0.2845818648943263, 0.11383274595773053, 0.17074911893659578, 0.17074911893659578, 0.056916372978865266, 0.5320331993177595, 0.26601659965887975, 0.5366027606835063, 0.26830138034175316, 0.40377519252784916, 0.40377519252784916, 0.13459173084261639, 0.07232833669193482, 0.16273875755685335, 0.19890292590282077, 0.18082084172983706, 0.05424625251895112, 0.18082084172983706, 0.14465667338386964, 0.4264057488140407, 0.4264057488140407, 0.21347969977883774, 0.6404390993365132, 0.10673984988941887, 0.4616854943988102, 0.0769475823998017, 0.0769475823998017, 0.0769475823998017, 0.0769475823998017, 0.0769475823998017, 0.1538951647996034, 0.21342979582344682, 0.21342979582344682, 0.32014469373517024, 0.10671489791172341, 0.10671489791172341, 0.22068540596055095, 0.22068540596055095, 0.4413708119211019, 0.5320265020916258, 0.2660132510458129, 0.2765604459424866, 0.2765604459424866, 0.2765604459424866, 0.5307481347725628, 0.35383208984837516, 0.18682100351629755, 0.18682100351629755, 0.18682100351629755, 0.3736420070325951, 0.41501270876123686, 0.02766751391741579, 0.08300254175224737, 0.11067005566966316, 0.13833756958707896, 0.08300254175224737, 0.02766751391741579, 0.08300254175224737, 0.02766751391741579, 0.21928244959160537, 0.6578473487748161, 0.5389465467947238, 0.4042099100960428, 0.6464828714784899, 0.21549429049283, 0.15641382000970705, 0.15641382000970705, 0.15641382000970705, 0.15641382000970705, 0.15641382000970705, 0.15641382000970705, 0.437304732274727, 0.2186523661373635, 0.2186523661373635, 0.2732885113671019, 0.5465770227342038, 0.40755936133294407, 0.20377968066647204, 0.152834760499854, 0.10188984033323602, 0.152834760499854, 0.4480219601072176, 0.2688131760643306, 0.17920878404288706, 0.08960439202144353, 0.2754134441390578, 0.41312016620858666, 0.1377067220695289, 0.1377067220695289, 0.15386534144186692, 0.23079801216280038, 0.23079801216280038, 0.15386534144186692, 0.07693267072093346, 0.15386534144186692, 0.15749205181740233, 0.15749205181740233, 0.47247615545220706, 0.15749205181740233, 0.5347581108325462, 0.2673790554162731, 0.8305179945283764, 0.3643413734205857, 0.18217068671029285, 0.3643413734205857, 0.05169036423636904, 0.3618325496545833, 0.05169036423636904, 0.10338072847273808, 0.2584518211818452, 0.1550710927091071, 0.5328718158511289, 0.17762393861704295, 0.17762393861704295, 0.21298128365272356, 0.4259625673054471, 0.21298128365272356, 0.17880207042183494, 0.35760414084366987, 0.17880207042183494, 0.17880207042183494, 0.5347442506703896, 0.2673721253351948, 0.07949569026109382, 0.3709798878851045, 0.07949569026109382, 0.15899138052218764, 0.15899138052218764, 0.10599425368145843, 0.026498563420364607, 0.13765992221690457, 0.27531984443380914, 0.13765992221690457, 0.41297976665071373, 0.538032177674945, 0.1793440592249817, 0.1793440592249817, 0.27474611213269523, 0.27474611213269523, 0.27474611213269523, 0.22033770242465617, 0.22033770242465617, 0.44067540484931234, 0.22003156017269737, 0.22003156017269737, 0.44006312034539474, 0.36317204606383313, 0.18158602303191657, 0.18158602303191657, 0.18158602303191657, 0.6337289592881757, 0.21124298642939193, 0.27624315802384886, 0.27624315802384886, 0.27624315802384886, 0.5285774057954883, 0.26428870289774414, 0.13214435144887207, 0.2749901135606734, 0.1374950567803367, 0.1374950567803367, 0.2749901135606734, 0.1374950567803367, 0.21701750987650592, 0.17960069782883248, 0.09728371132395093, 0.029933449638138748, 0.08231698650488156, 0.17960069782883248, 0.08980034891441624, 0.059866899276277495, 0.037416812047673435, 0.02245008722860406, 0.27628611137168435, 0.27628611137168435, 0.27628611137168435, 0.2870103548122109, 0.5740207096244218, 0.22011915980497596, 0.22011915980497596, 0.4402383196099519, 0.09578617461933533, 0.09578617461933533, 0.33525161116767366, 0.28735852385800603, 0.09578617461933533, 0.04789308730966767, 0.27519142388419393, 0.27519142388419393, 0.27519142388419393, 0.3991339732846591, 0.3991339732846591, 0.13304465776155303, 0.13304465776155303, 0.36977846547168597, 0.15847648520215113, 0.10565099013476742, 0.15847648520215113, 0.01760849835579457, 0.08804249177897286, 0.08804249177897286, 0.01760849835579457, 0.2717737540535625, 0.2717737540535625, 0.2717737540535625, 0.6421391615957807, 0.21404638719859356, 0.212100929995519, 0.212100929995519, 0.424201859991038, 0.3790335432527049, 0.18951677162635244, 0.3790335432527049, 0.17739337162717592, 0.5321801148815277, 0.17739337162717592, 0.22516607070725647, 0.22516607070725647, 0.0900664282829026, 0.13509964242435388, 0.06754982121217694, 0.0900664282829026, 0.11258303535362824, 0.02251660707072565, 0.2739766375191337, 0.13698831875956685, 0.13698831875956685, 0.13698831875956685, 0.2739766375191337, 0.2702087469503046, 0.5404174939006092, 0.7960515739026441, 0.1888950763026232, 0.0472237690756558, 0.14167130722696739, 0.28334261445393477, 0.0944475381513116, 0.0472237690756558, 0.0944475381513116, 0.0472237690756558, 0.4965911913163784, 0.16553039710545947, 0.16553039710545947, 0.16553039710545947, 0.564986321832876, 0.282493160916438, 0.13698205499907554, 0.41094616499722664, 0.2739641099981511, 0.3815285692223788, 0.13626020329370672, 0.2588943862580428, 0.05450408131748269, 0.08175612197622403, 0.013626020329370672, 0.013626020329370672, 0.027252040658741344, 0.013626020329370672, 0.013626020329370672, 0.4336855828254566, 0.2168427914127283, 0.2168427914127283, 0.2701507225363893, 0.2701507225363893, 0.2701507225363893], \"Term\": [\"abl\", \"abl\", \"abl\", \"abl\", \"abl\", \"abl\", \"abl\", \"account\", \"account\", \"account\", \"accur\", \"accur\", \"activ\", \"activ\", \"actual\", \"actual\", \"actual\", \"actual\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"allow\", \"allow\", \"allow\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"also\", \"alway\", \"alway\", \"alway\", \"ask\", \"ask\", \"assign\", \"assign\", \"assist\", \"assist\", \"assist\", \"assist\", \"attend\", \"attend\", \"avail\", \"avail\", \"avail\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"behind\", \"behind\", \"believ\", \"believ\", \"believ\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"best\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"better\", \"better\", \"bia\", \"bia\", \"bia\", \"bia\", \"bia\", \"bia\", \"bia\", \"bias\", \"bias\", \"bias\", \"care\", \"care\", \"care\", \"case\", \"case\", \"case\", \"case\", \"case\", \"certain\", \"certain\", \"certain\", \"certain\", \"certain\", \"chanc\", \"chanc\", \"chanc\", \"chanc\", \"chanc\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"children\", \"children\", \"children\", \"children\", \"children\", \"children\", \"choic\", \"choic\", \"choic\", \"choos\", \"choos\", \"choos\", \"circumst\", \"circumst\", \"class\", \"class\", \"class\", \"combin\", \"combin\", \"combin\", \"combin\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comput\", \"comput\", \"comput\", \"comput\", \"consider\", \"consider\", \"consider\", \"control\", \"control\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"counsel\", \"counsel\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"counselor\", \"creat\", \"creat\", \"creat\", \"cutoff\", \"cutoff\", \"cutoff\", \"cutoff\", \"cutoff\", \"cutoff\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"decid\", \"decid\", \"decid\", \"decid\", \"decis\", \"decis\", \"decis\", \"decis\", \"determin\", \"determin\", \"determin\", \"determin\", \"determin\", \"differ\", \"differ\", \"draw\", \"draw\", \"draw\", \"draw\", \"drawn\", \"drawn\", \"due\", \"due\", \"educ\", \"educ\", \"effici\", \"effici\", \"effici\", \"elimin\", \"elimin\", \"emot\", \"emot\", \"emot\", \"emot\", \"enough\", \"enough\", \"enough\", \"equal\", \"error\", \"error\", \"etc\", \"etc\", \"etc\", \"even\", \"even\", \"even\", \"even\", \"everi\", \"everi\", \"everi\", \"everyon\", \"fact\", \"fact\", \"fact\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"fail\", \"fail\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fair\", \"fall\", \"fall\", \"famili\", \"famili\", \"famili\", \"famili\", \"far\", \"far\", \"far\", \"favor\", \"favor\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"figur\", \"figur\", \"figur\", \"find\", \"find\", \"find\", \"futur\", \"futur\", \"gener\", \"gener\", \"gener\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"give\", \"given\", \"given\", \"given\", \"good\", \"good\", \"good\", \"good\", \"hard\", \"hard\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"help\", \"home\", \"home\", \"howev\", \"howev\", \"howev\", \"human\", \"human\", \"human\", \"human\", \"human\", \"identifi\", \"identifi\", \"identifi\", \"import\", \"import\", \"improv\", \"improv\", \"incom\", \"incom\", \"incom\", \"incom\", \"incom\", \"incom\", \"incom\", \"incom\", \"individu\", \"individu\", \"inform\", \"inform\", \"inform\", \"inform\", \"inform\", \"involv\", \"involv\", \"involv\", \"issu\", \"issu\", \"issu\", \"judg\", \"judg\", \"judgement\", \"judgement\", \"judgement\", \"judgement\", \"judgement\", \"judgement\", \"judgment\", \"judgment\", \"judgment\", \"judgment\", \"keep\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"kid\", \"know\", \"know\", \"know\", \"know\", \"know\", \"lead\", \"lead\", \"least\", \"least\", \"less\", \"less\", \"less\", \"less\", \"life\", \"life\", \"life\", \"life\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"limit\", \"line\", \"line\", \"line\", \"live\", \"live\", \"live\", \"live\", \"live\", \"lot\", \"lot\", \"lot\", \"lot\", \"low\", \"low\", \"made\", \"made\", \"made\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"make\", \"mani\", \"mani\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"may\", \"mayb\", \"mayb\", \"mayb\", \"mean\", \"mean\", \"mean\", \"mean\", \"mean\", \"measur\", \"measur\", \"measur\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"mentor\", \"method\", \"method\", \"method\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"might\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"money\", \"money\", \"money\", \"much\", \"much\", \"much\", \"much\", \"much\", \"much\", \"name\", \"name\", \"name\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"need\", \"neither\", \"neither\", \"number\", \"number\", \"object\", \"object\", \"object\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"one\", \"opinion\", \"opinion\", \"opportun\", \"opportun\", \"other\", \"other\", \"other\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"past\", \"past\", \"peopl\", \"peopl\", \"peopl\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"person\", \"pick\", \"pick\", \"pick\", \"pick\", \"pick\", \"play\", \"play\", \"play\", \"plenti\", \"plenti\", \"point\", \"point\", \"point\", \"poor\", \"poor\", \"possibl\", \"possibl\", \"possibl\", \"possibl\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predictor\", \"predictor\", \"probabl\", \"probabl\", \"problem\", \"problem\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"provid\", \"provid\", \"provid\", \"put\", \"put\", \"random\", \"random\", \"random\", \"random\", \"random\", \"randomli\", \"randomli\", \"randomli\", \"randomli\", \"rather\", \"rather\", \"rather\", \"rather\", \"realli\", \"realli\", \"realli\", \"realli\", \"realli\", \"realli\", \"receiv\", \"receiv\", \"receiv\", \"receiv\", \"regardless\", \"regardless\", \"relat\", \"remov\", \"remov\", \"remov\", \"request\", \"request\", \"request\", \"request\", \"request\", \"request\", \"resourc\", \"resourc\", \"resourc\", \"result\", \"result\", \"result\", \"risk\", \"risk\", \"risk\", \"risk\", \"say\", \"say\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"school\", \"select\", \"select\", \"select\", \"select\", \"sens\", \"sens\", \"sens\", \"servic\", \"servic\", \"servic\", \"set\", \"set\", \"set\", \"show\", \"show\", \"show\", \"situat\", \"situat\", \"situat\", \"situat\", \"someth\", \"someth\", \"statist\", \"statist\", \"statist\", \"still\", \"still\", \"still\", \"struggl\", \"struggl\", \"struggl\", \"struggl\", \"struggl\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"success\", \"success\", \"success\", \"sure\", \"sure\", \"system\", \"system\", \"system\", \"take\", \"take\", \"take\", \"take\", \"take\", \"take\", \"taken\", \"taken\", \"taken\", \"thing\", \"thing\", \"thing\", \"thing\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"think\", \"though\", \"though\", \"though\", \"thought\", \"thought\", \"time\", \"time\", \"time\", \"truli\", \"truli\", \"truli\", \"unfair\", \"unfair\", \"unfair\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"variabl\", \"variabl\", \"variabl\", \"variabl\", \"variabl\", \"varieti\", \"varieti\", \"want\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"way\", \"well\", \"well\", \"well\", \"well\", \"whole\", \"whole\", \"without\", \"without\", \"without\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"would\", \"wrong\", \"wrong\", \"wrong\", \"year\", \"year\", \"year\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 7, 9, 2, 10, 5, 3, 1, 6, 8]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el770591404897043557608363110774\", ldavis_el770591404897043557608363110774_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el770591404897043557608363110774\", ldavis_el770591404897043557608363110774_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el770591404897043557608363110774\", ldavis_el770591404897043557608363110774_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words = 20)\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_display = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics not capturing more nuanced moral differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "## see from viz that, for instance, topic 10 is focused on issues of counselor bias, topic 2 on issues with parent\n",
    "## requests\n",
    "\n",
    "## get document-specific topic probabilities\n",
    "## has element for each document w/ tuples of topic probabilities\n",
    "all_topics = ldamodel.get_document_topics(corpus, minimum_probability=0.0, per_word_topics=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an example response focused on algorithms as efficient, these are the topic probabilities:\n",
      "----------------------------------------------------------------------------\n",
      "I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.\n",
      "[(0, 0.001819176), (1, 0.0023839534), (2, 0.0020805404), (3, 0.0029978217), (4, 0.0022326568), (5, 0.0018926469), (6, 0.002496051), (7, 0.0016892517), (8, 0.98022133), (9, 0.0021865289)]\n",
      "For this efficiency example, highest-probability topic is topic 8\n"
     ]
    }
   ],
   "source": [
    "efficient_nostories = frdata_schools.explain_fairness[frdata_schools.explain_fairness.astype(str).str.contains(\"stories\")]\n",
    "print(\"For an example response focused on algorithms as efficient, these are the topic probabilities:\")\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(efficient_nostories.to_string(index = False))\n",
    "print(all_topics[efficient_nostories.index[0]])\n",
    "focal_text = efficient_nostories.index\n",
    "\n",
    "## get index of top topic \n",
    "top_topic = sorted(all_topics[efficient_nostories.index[0]], key = lambda x: x[1])[len(test)-1][0]\n",
    "print(\"For this efficiency example, highest-probability topic is topic \" + str(top_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>binary_morefair</th>\n",
       "      <th>explain_fairness</th>\n",
       "      <th>pol_ideology</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>The predictive model is more fair because the alternative, where the counselor decides based on parents' requests, means that some kids who need help won't get help because their parents may not be active in their lives enough to go out and request help. It also removes the chances of some kind of personal bias on the part of a counselor clouding their judgment.</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>Random drawing for things like this are not the best way to do things due to the fact that a lot of kids don't need or want one on one counseling so they are taking up a spot from someone who actually needs it.  Using a 100% algorithm model though is also not completly fair due to the fact that there will be outliners that get skipped and just because a stupid looks like they are \"ok\" on paper, that may not actually be the case in real life.  The most fair way to do it would be a combination of the two system with something like 80% picked from model and then maybe 20% done through a stupid signup sheet (if there are too many then randomly draw from that 20%).</td>\n",
       "      <td>Slightly conservative</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>Some parents who don't really need it quite as much might send a request and some parents who do need it might be too proud to send a request. with this model it would better determine who needs it more.</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>Income cutoff could be pretty subjective. Lets say you cut it off at 15000 but tons of students who come from 16000 income are having just as many issues and don't get help. Need to mix a variety of metrics to figure out who needs help.</td>\n",
       "      <td>Extremely Conservative</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## see topic 0 is the top topic--- look at other responses with high-probability of that topic\n",
    "## empty topic dictionary\n",
    "topic_dict = {i: [] for i in range(len(all_topics[top_topic]))}  \n",
    "\n",
    "## iterate over docs and append the vector of \n",
    "## document-specific probabilities to each key (one topic)\n",
    "for docID in range(len(all_topics)):\n",
    "    topic_vector = ldamodel[corpus[docID]]\n",
    "    for topicID, prob in topic_vector:\n",
    "        topic_dict[topicID].append([docID, prob])\n",
    "     \n",
    "## pull out that topic and pull topic responses\n",
    "docs_fortopic = topic_dict[top_topic]\n",
    "indices_top_5_resp = [el[0] for el in \n",
    "                                sorted(docs_fortopic, key=itemgetter(1))[len(docs_fortopic)-5:len(docs_fortopic)]]\n",
    "\n",
    "\n",
    "## see that they're either talking about benefits of a predictive model, \n",
    "## esp relative to randomness, but not the focal responses ideas about\n",
    "## efficiency (regardless of its normative desirability)\n",
    "display(HTML(pd.DataFrame(frdata_schools.loc[frdata_schools.explain_fairness.index.isin(indices_top_5_resp),\n",
    "                                ['binary_morefair',\n",
    "                                'explain_fairness',\n",
    "                                'pol_ideology',\n",
    "                                'gender']]).to_html(index = \\\n",
    "            False)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: dictionary-based method using moral foundations theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create input file that matches formatting\n",
    "frdata_schools.explain_fairness.to_csv(\"fr_forscore.csv\",\n",
    "                                      index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eMFDscore\n",
      "Total number of input texts to be scored: 212\n",
      "Scoring completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Processed: 0 N/A% |                      | Elapsed Time: 0:00:00 ETA:  --:--:--\r",
      "Processed: 212 100% |❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤❤| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "emfdscore fr_forscore.csv school_mftscores.csv bow emfd all vice-virtue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For an example response focused on algorithms as efficient, these are the moral foundation scores:\n",
      "----------------------------------------------------------------------------\n",
      "I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>care.virtue</th>\n",
       "      <th>fairness.virtue</th>\n",
       "      <th>loyalty.virtue</th>\n",
       "      <th>authority.virtue</th>\n",
       "      <th>sanctity.virtue</th>\n",
       "      <th>care.vice</th>\n",
       "      <th>fairness.vice</th>\n",
       "      <th>loyalty.vice</th>\n",
       "      <th>authority.vice</th>\n",
       "      <th>sanctity.vice</th>\n",
       "      <th>moral_nonmoral_ratio</th>\n",
       "      <th>f_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.041613</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>0.039836</td>\n",
       "      <td>0.02459</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.033713</td>\n",
       "      <td>0.056424</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     care.virtue  fairness.virtue  loyalty.virtue  authority.virtue  \\\n",
       "129  0.04         0.041613         0.061589        0.025956           \n",
       "\n",
       "     sanctity.virtue  care.vice  fairness.vice  loyalty.vice  authority.vice  \\\n",
       "129  0.011745         0.039836   0.02459        0.022727      0.033713         \n",
       "\n",
       "     sanctity.vice  moral_nonmoral_ratio     f_var  \n",
       "129  0.056424       4.0                   0.000237  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## pull up results for focal text\n",
    "## see that it's generally low in all\n",
    "## but is highest in loyalty,\n",
    "## somewhat high in virtue\n",
    "mfdt_res = pd.read_csv(\"school_mftscores.csv\")\n",
    "focal_mfdt = mfdt_res.iloc[focal_text]\n",
    "print(\"For an example response focused on algorithms as efficient, these are the moral foundation scores:\")\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "print(efficient_nostories.to_string(index = False))\n",
    "focal_mfdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add index as row number to each\n",
    "frdata_schools['doc'] = frdata_schools.index\n",
    "mfdt_res['doc'] = mfdt_res.index\n",
    "\n",
    "mfdt_wanswer = pd.merge(frdata_schools,\n",
    "                       mfdt_res,\n",
    "                       on = \"doc\",\n",
    "                       how = \"left\")\n",
    "\n",
    "## shows other answers that score similarly high in virtue \n",
    "## but that touch on very different moral concerns\n",
    "prox_answer = mfdt_wanswer.sort_values(by = 'loyalty.virtue', ascending = False).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>binary_morefair</th>\n",
       "      <th>explain_fairness</th>\n",
       "      <th>loyalty.virtue</th>\n",
       "      <th>pol_ideology</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>I think it would be easier to apply predictive models than learning about each individual student. It would be easier in the sense of using time efficiently so you can create a school program or class that would help more students in a similar situation. It takes too much time to sit down with each student in order to know their individual stories.</td>\n",
       "      <td>0.061589</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>I think it would be fair.</td>\n",
       "      <td>0.061347</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>I think it is more fair because students call fall into certain algorithms and models.</td>\n",
       "      <td>0.061264</td>\n",
       "      <td>Slightly conservative</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>It helps gauge the general state of the class as a whole</td>\n",
       "      <td>0.061202</td>\n",
       "      <td>Moderate, Middle of road</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>school counselor using a predictive model/algorithm</td>\n",
       "      <td>If initially set up correctly, all students will me measured using the same algorithm. There wouldn't be any chance that one student is more favored than the other other than based on their needs.</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(pd.DataFrame(prox_answer.loc[prox_answer.index.isin(range(prox_answer[prox_answer.doc == focal_text[0]].index[0],\n",
    "                                        prox_answer[prox_answer.doc == focal_text[0]].index[0]+5)),\n",
    "            ['binary_morefair',\n",
    "            'explain_fairness',\n",
    "            'loyalty.virtue',\n",
    "            'pol_ideology',\n",
    "            'gender']]).to_html(index = False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
